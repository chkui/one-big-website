webpackJsonp([11],{399:function(a,s,t){"use strict";Object.defineProperty(s,"__esModule",{value:!0});s.content='<h2 id="h2-1">非侵入式框架</h2>\n<p>\n    Spring一直标注自己是一个非侵入式框架。非侵入式设计的概念并不新鲜，目标就是降低使用者和框架代码的耦合，毕竟框架的开发者和使用者几乎肯定不是同一个团队。Spring最早的非侵入式实现就是他的一系列XML配置，理想状态下Spring框架的所有的功能都应该是通过配置实现的。元编程在Java中的使用现给非侵入式的设计提供了更好的解决方案，在Java中通过注解（Annotation）即可标记某个类、方法、域的附加功能，而无需通过继承的方式来扩展原始框架没有的功能。下面通过3段代码的例子来说明侵入式与非侵入式的区别。</p>\n<p><span style="color:#e74c3c">文章中的代码仅仅用于说明原理，已经删除了一些无关代码，无法执行。可执行代码在：<a\n        href="https://github.com/chkui/spring-core-example"\n        rel="nofollow">https://github.com/chkui/spring-core-example</a>，如有需要请自行clone，仅支持gradle依赖。</span></p>\n\n<h3 id="h3-1">一个基本的容器</h3>\n<p>下面的代码是大致模仿的IoC容器创建Bean的过程。BeanFactory::createBeans方法传入Bean的类型列表，而迭代器遍历列表完成每一个类的实例创建：</p>\n<pre><code class="java"><span class="code-comment">/**框架代码*/</span>\n<span class="code-keyword">package</span> chkui.springcore.example.xml.beanpostprocessor.nopluging;\n\n<span class="code-comment">//创建Bean的工厂类,由框架开发者开发</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">BeanFactory</span> </span>{\n\t<span class="code-comment">//创建一系列的Bean</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> List&lt;Object&gt; <span class="code-title">createBeans</span><span\n            class="hljs-params">(List&lt;Class&lt;?&gt;&gt; clslist)</span></span>{\n\t\t<span class="code-keyword">return</span> clslist.stream().map(cls-&gt;{\n\t\t\t<span class="code-keyword">return</span> createBean(cls);\n\t\t}).collect(Collectors.toList());\n\t}\n\t<span class="code-comment">//创建一个Bean</span>\n\t<span class="hljs-function">Object <span class="code-title">createBean</span><span class="hljs-params">(Class&lt;?&gt; cls)</span></span>{\n\t\t<span class="code-comment">//添加到容器</span>\n\t\t<span class="code-keyword">return</span> <span class="code-keyword">new</span> BeanWrapper(cls.newInstance());\n\t}\n}\n\n<span class="code-comment">//包装代理</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">BeanWrapper</span> </span>{\n\t<span class="code-keyword">private</span> Object bean;\n\t<span class="hljs-function"><span class="code-keyword">public</span> <span\n            class="code-title">BeanWrapper</span><span class="hljs-params">(Object bean)</span> </span>{\n\t\t<span class="code-keyword">this</span>.bean = bean;\n\t}\n\t<span class="code-meta">@Override</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> String <span class="code-title">toString</span><span\n            class="hljs-params">()</span> </span>{\n\t\t<span class="code-keyword">return</span> <span class="code-string">"Wrapper("</span> + <span\n            class="code-keyword">this</span>.bean.toString() + <span class="code-string">")"</span>;\n\t}\n}</code></pre>\n<p>下面的代码是框架使用者的代码——将Bean1和Bean2交给BeanFactory来完成初始化：</p>\n<pre><code class="java"><span class="code-comment">/**使用端代码*/</span>\n<span class="code-keyword">package</span> chkui.springcore.example.xml.beanpostprocessor.nopluging;\n\n<span class="code-comment">//import ...</span>\n\n<span class="code-keyword">public</span> <span class="hljs-class"><span class="code-keyword">class</span> <span\n            class="code-title">IocExtensionSampleNoPluging</span> </span>{\n    <span class="hljs-function"><span class="code-keyword">public</span> <span class="code-keyword">static</span> <span\n            class="code-keyword">void</span> <span class="code-title">main</span><span class="hljs-params">(String[] args)</span> </span>{\n    \tList&lt;Class&lt;?&gt;&gt; classes = Arrays.asList(<span class="code-keyword">new</span> Class&lt;?&gt;[]{MyBean1.class, MyBean2.class});\n    \tList&lt;Object&gt; ins = <span class="code-keyword">new</span> BeanFactory().createBeans(classes);\n    \tSystem.out.println(<span class="code-string">"Result:"</span> + ins.toString());\n    }\n}\n\n<span class="code-comment">//Bean1，由使用者编码</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">MyBean1</span> </span>{\n\t<span class="hljs-function"><span class="code-keyword">public</span> String <span class="code-title">toString</span><span\n            class="hljs-params">()</span> </span>{\n\t\t<span class="code-keyword">return</span> <span class="code-string">"MyBean1 Ins"</span>;\n\t}\n}\n\n<span class="code-comment">//Bean2，使用者编码</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">MyBean2</span> </span>{\n\t<span class="hljs-function"><span class="code-keyword">public</span> String <span class="code-title">toString</span><span\n            class="hljs-params">()</span> </span>{\n\t\t<span class="code-keyword">return</span> <span class="code-string">"MyBean2 Ins"</span>;\n\t}\n}</code></pre>\n<p>classpath：chkui.springcore.example.xml.beanpostprocessor.nopluging.IocExtensionSample。<a\n        href="https://github.com/chkui/spring-core-example/blob/master/src/main/java/chkui/springcore/example/xml/beanpostprocessor/nopluging/IocExtensionSample.java"\n        rel="nofollow">源码地址</a>。</p>\n<p>某个时刻，框架的使用者有个新需求是在要在每个Bean创建的前后进行一些处理。我们可以通过继承的方式来实现功能。下面我们修改使用端代码实现这个功能。</p>\n\n<h3 id="h3-2">继承实现功能扩展</h3>\n<p>通过继承类BeanFactory，并修改createBean方法可以实现我们的需求：</p>\n<pre><code class="java"><span class="code-keyword">package</span> chkui.springcore.example.xml.beanpostprocessor.extend;\n\n<span class="code-comment">//执行</span>\n<span class="code-keyword">public</span> <span class="hljs-class"><span class="code-keyword">class</span> <span\n            class="code-title">IocExtensionSampleNoPluging</span> </span>{\n    <span class="hljs-function"><span class="code-keyword">public</span> <span class="code-keyword">static</span> <span\n            class="code-keyword">void</span> <span class="code-title">main</span><span class="hljs-params">(String[] args)</span> </span>{\n    \tList&lt;Class&lt;?&gt;&gt; classes = Arrays.asList(<span class="code-keyword">new</span> Class&lt;?&gt;[]{MyBean1.class, MyBean2.class});\n    \tList&lt;Object&gt; ins = <span class="code-keyword">new</span> ModifyBeanFactory().createBeans(classes);\n    \tSystem.out.println(<span class="code-string">"Result:"</span> + ins.toString());\n    }\n}\n\n<span class="code-comment">//新建一个BeanFactory的派生类，并修改createBean的实现，添加使用者的处理逻辑</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">ModifyBeanFactory</span> <span\n        class="code-keyword">extends</span> <span class="code-title">BeanFactory</span> </span>{\n\t<span class="hljs-function">Object <span class="code-title">createBean</span><span class="hljs-params">(Class&lt;?&gt; cls)</span></span>{\n\t\tObject ins = cls.newInstance();\n\t\t<span class="code-comment">//添加容器之前的处理</span>\n\t\tBeanWrapper wrapper = <span class="code-keyword">new</span> BeanWrapper(ins);\n\t\t<span class="code-comment">//添加容器之后的处理</span>\n\t\t<span class="code-keyword">return</span> wrapper;\n\t}\n}</code></pre>\n<p>classpath：chkui.springcore.example.xml.beanpostprocessor.extend.IocExtensionSample。<a\n        href="https://github.com/chkui/spring-core-example/blob/master/src/main/java/chkui/springcore/example/xml/beanpostprocessor/extend/IocExtensionSample.java"\n        rel="nofollow">源码地址</a>。</p>\n<p>这里在使用者的代码里新增了一个ModifyBeanFactory类，并重写了createBean方法。在重写的方法中实现我们需要的功能逻辑。但是这样开发会出现以下2点问题：</p>\n<ol>\n    <li>导致使用者的代码与框架代码产生了极强的耦合性。如果某天框架进行了调整，例如将方法名改为buildBean、或者增加了更多的代理模式会出现一些意想不到的问题。更麻烦的是可能会遇到一些到运行期才出现的问题。</li>\n    <li>我们需要先理解框架的源码才能植入我们的功能，这和很多设计模式的原则是背道而驰的。也会大大影响我们的开发效率。</li>\n</ol>\n<p>出现这些问题就叫做“侵入式”——框架代码侵入到使用者的工程代码，导致2者严重耦合，对未来的升级、扩展、二次开发都有深远的影响。</p>\n\n<h2 id="h2-2">通过注解（Annotation）扩展功能</h2>\n<p>实际上注解和在XML进行配置都是一样的思路，只是注解讲关系写在了源码上，而使用XML是将关系通过XML来描述。这里实现的功能就类似于在<a\n        href="https://www.chkui.com/article/spring/spring_core_bean_lifecycle_callback"\n        rel="nofollow">&nbsp;Bean的定义与控制</a>&nbsp;一文中介绍的Bean的生命周期方法。</p>\n<p>使用注解最大的价值就是非侵入式。非侵入式的好处显而易见：</p>\n<ol>\n    <li>无需和框架代码耦合，更新升级框架风险和成本都很小。</li>\n    <li>任何时候我们需要需要更换框架，只需修改配置或注解，而无需再去调整我们自己的功能代码。</li>\n</ol>\n<p>非侵入式也有一个问题，那就是接入的功能还是需要框架预设，而不可能像继承那样随心所欲。</p>\n<p>我们将前面的代码进行一些修改，支持通过注解来指定扩展的功能：</p>\n<pre><code class="java"><span class="code-keyword">package</span> chkui.springcore.example.xml.beanpostprocessor.annotation;\n\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">BeanFactory</span> </span>{\n\t<span class="hljs-function"><span class="code-keyword">public</span> List&lt;Object&gt; <span class="code-title">createBeans</span><span\n            class="hljs-params">(List&lt;Class&lt;?&gt;&gt; clslist)</span></span>{\n\t\t<span class="code-comment">//同前文...</span>\n\t}\n\t<span class="hljs-function">Object <span class="code-title">createBean</span><span class="hljs-params">(Class&lt;?&gt; cls)</span></span>{\n\t\tBeanWrapper wrapper = <span class="code-keyword">null</span>;\n\t\tObject ins = cls.newInstance();\n        <span class="code-comment">/**这里增加了一个Handle对象。\n           Handle会对注解进行处理，确定添加容器前后的执行方法。*/</span>\n\t\tHandle handle = processBeforeAndAfterHandle(ins);\n\t\thandle.exeBefore();\n\t\twrapper = <span class="code-keyword">new</span> BeanWrapper(ins);\n\t\thandle.exeAfter();\n\t\t<span class="code-keyword">return</span> wrapper;\n\t}\n\t\n    <span class="code-comment">// 通过反射来确定Bean被添加到容器前后的执行方法。</span>\n\t<span class="hljs-function"><span class="code-keyword">private</span> Handle <span class="code-title">processBeforeAndAfterHandle</span><span\n            class="hljs-params">(Object obj)</span> </span>{\n\t\tMethod[] methods = obj.getClass().getDeclaredMethods();\n\t\tHandle handle = <span class="code-keyword">new</span> Handle(obj);\n\t\t<span class="code-keyword">for</span>(Method method : methods) {\n\t\t\tAnnotation bef = method.getAnnotation(before.class);\n\t\t\tAnnotation aft = method.getAnnotation(after.class);\n\t\t\t<span class="code-keyword">if</span>(<span class="code-keyword">null</span> != bef) handle.setBefore(method);\n\t\t\t<span class="code-keyword">if</span>(<span class="code-keyword">null</span> != aft) handle.setBefore(method);\n\t\t}\n\t\t<span class="code-keyword">return</span> handle;\n\t}\n}</code></pre>\n<p>下面是Handle处理器和对应的注解的代码：</p>\n<pre><code class="java"><span class="hljs-class"><span class="code-keyword">class</span> <span\n        class="code-title">Handle</span></span>{\n\tObject instance;\n\tMethod before;\n\tMethod after;\n\tHandle(Object ins){\n\t\t<span class="code-keyword">this</span>.instance = ins;\n\t}\n\t<span class="hljs-function"><span class="code-keyword">void</span> <span class="code-title">setBefore</span><span\n            class="hljs-params">(Method method)</span> </span>{\n\t\t<span class="code-keyword">this</span>.before = method;\n\t}\n\t<span class="hljs-function"><span class="code-keyword">void</span> <span class="code-title">setAfter</span><span\n            class="hljs-params">(Method method)</span> </span>{\n\t\t<span class="code-keyword">this</span>.after = method;\n\t}\n\t<span class="hljs-function"><span class="code-keyword">void</span> <span class="code-title">exeBefore</span><span\n            class="hljs-params">()</span></span>{\n\t\t<span class="code-keyword">if</span>(<span class="code-keyword">null</span> != <span\n            class="code-keyword">this</span>.before) {\n\t\t\t<span class="code-keyword">this</span>.before.invoke(<span class="code-keyword">this</span>.instance, <span\n            class="code-keyword">null</span>);\n\t\t}\n\t}\n\t<span class="hljs-function"><span class="code-keyword">void</span> <span class="code-title">exeAfter</span><span\n            class="hljs-params">()</span></span>{\n\t\t<span class="code-keyword">if</span>(<span class="code-keyword">null</span> != <span\n            class="code-keyword">this</span>.after) {\n\t\t\t<span class="code-keyword">this</span>.after.invoke(<span class="code-keyword">this</span>.instance, <span\n            class="code-keyword">null</span>);\n\t\t}\n\t}\n}\n\n<span class="code-comment">//注解----------------------------------------</span>\n<span class="code-meta">@Target</span>({ElementType.METHOD})\n<span class="code-meta">@Retention</span>(RetentionPolicy.RUNTIME)\n<span class="code-meta">@interface</span> before {}\n\n<span class="code-meta">@Target</span>({ElementType.METHOD})\n<span class="code-meta">@Retention</span>(RetentionPolicy.RUNTIME)\n<span class="code-meta">@interface</span> after{}</code></pre>\n<p>使用者的代码，我们将注解添加到Bean的对应的方法上：</p>\n<pre><code class="java"><span class="code-keyword">public</span> <span class="hljs-class"><span class="code-keyword">class</span> <span\n        class="code-title">IocExtensionSampleNoPluging</span> </span>{\n    <span class="hljs-function"><span class="code-keyword">public</span> <span class="code-keyword">static</span> <span\n            class="code-keyword">void</span> <span class="code-title">main</span><span class="hljs-params">(String[] args)</span> </span>{\n    \tList&lt;Class&lt;?&gt;&gt; classes = Arrays.asList(<span class="code-keyword">new</span> Class&lt;?&gt;[]{MyBean1.class, MyBean2.class});\n    \tList&lt;Object&gt; ins = <span class="code-keyword">new</span> BeanFactory().createBeans(classes);\n    \tSystem.out.println(<span class="code-string">"Result:"</span> + ins.toString());\n    }\n}\n\n<span class="code-comment">//预设的Bean1</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">MyBean1</span> </span>{\n\t<span class="hljs-function"><span class="code-keyword">public</span> String <span class="code-title">toString</span><span\n            class="hljs-params">()</span> </span>{\n\t\t<span class="code-keyword">return</span> <span class="code-string">"MyBean1 Ins"</span>;\n\t}\n\t\n\t<span class="code-meta">@before</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> <span class="code-keyword">void</span> <span\n            class="code-title">init</span><span class="hljs-params">()</span> </span>{\n    \tSystem.out.println(<span class="code-string">"Before Init:"</span> + <span class="code-keyword">this</span>.toString());\n\t}\n}\n\n<span class="code-comment">//预设的Bean2</span>\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">MyBean2</span> </span>{\n\t<span class="hljs-function"><span class="code-keyword">public</span> String <span class="code-title">toString</span><span\n            class="hljs-params">()</span> </span>{\n\t\t<span class="code-keyword">return</span> <span class="code-string">"MyBean2 Ins"</span>;\n\t}\n\t\n\t<span class="code-meta">@after</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> <span class="code-keyword">void</span> <span\n            class="code-title">post</span><span class="hljs-params">()</span> </span>{\n    \tSystem.out.println(<span class="code-string">"After Init:"</span> + <span class="code-keyword">this</span>.toString());\n\t}\n}</code></pre>\n<p>我们为MyBean1和MyBean2分别添加了init、post方法和对应的@before、@after注解。执行之后输出一下内容：</p>\n<pre><code class="apache"><span class="code-attribute">Before</span> Init:MyBean1 Ins\n<span class="code-attribute">After</span> Init:MyBean2 Ins\n<span class="code-attribute">Result</span>:[Wrapper(MyBean1 Ins), Wrapper(MyBean2 Ins)]</code></pre>\n<p>classpath：chkui.springcore.example.xml.beanpostprocessor.annotation.IocExtensionSample。<a\n        href="https://github.com/chkui/spring-core-example/blob/master/src/main/java/chkui/springcore/example/xml/beanpostprocessor/annotation/IocExtensionSample.java"\n        rel="nofollow">源码地址</a>。</p>\n<p>注解对应的方法都顺利执行。</p>\n<p>通过注解，我们实现了扩展功能，任何时候只需要通过添加或修改注解即可向容器扩展功能。在Spring核心功能里，<a\n        href="http://www.chkui.com/article/spring/spring_core_bean_lifecycle_callback" title="Bean的生命周期管理">Bean的生命周期管理</a>都是通过这种思路实现的，除了注解之外还有XML支持。\n</p>\n<p>\n    在使用spring的过程中，我想各位码友多多少少都通过继承Spring某些类来实现了一些需要扩展的功能。而且我发现网上很多使用spring某些功能的例子也是通过继承实现的。建议尽量不要去采用这种加深耦合的方式实现扩展，Spring提供了多种多样的容器扩展机制，后面的文章会一一介绍。</p>\n\n<h2 id="h2-3">后置处理器</h2>\n<p>\n    后置处理器——BeanPostProcessor是Spring核心框架容器扩展功能之一，作用和Bean的生命周期方法类似，也是在Bean完成初始化前后被调用。但是和生命周期方法不同的是，他无需在每一个Bean上去实现代码，而是通过一个独立的Bean来处理全局的初始化过程。</p>\n<p>BeanPostProcessor与Bean生命周期方法体现出的差异是：<span style="color:#e74c3c">我们无论任何时候都可以加入处理器来实现扩展功能，这样做的好处是无需调整之前的Bean的任何代码也可以植入功能</span>。\n</p>\n<p>这种实现方式与切面（AOP）有一些相似的地方，但是实现的方式是完全不一样的，而且处理器会对所有Bean进行处理。</p>\n<p>BeanPostProcessor的实现非常简单，只添加一个Bean实现BeanPostProcessor接口即可：</p>\n<pre><code class="java"><span class="code-keyword">package</span> chkui.springcore.example.xml.beanpostprocessor;\n<span class="code-keyword">import</span> org.springframework.beans.factory.config.BeanPostProcessor;\n\n<span class="code-keyword">public</span> <span class="hljs-class"><span class="code-keyword">class</span> <span\n            class="code-title">Processor</span> <span class="code-keyword">implements</span> <span class="code-title">BeanPostProcessor</span> </span>{\n    <span class="code-comment">//初始化之前</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> Object <span class="code-title">postProcessBeforeInitialization</span><span\n            class="hljs-params">(Object bean, String beanName)</span> </span>{\n        <span class="code-keyword">return</span> bean;\n    }\n\t<span class="code-comment">//初始化之后</span>\n\t<span class="hljs-function"><span class="code-keyword">public</span> Object <span class="code-title">postProcessAfterInitialization</span><span\n            class="hljs-params">(Object bean, String beanName)</span> </span>{\n        System.out.println(<span class="code-string">"Bean \'"</span> + beanName + <span class="code-string">"\' created : "</span> + bean.toString());\n        <span class="code-keyword">return</span> bean;\n    }\n}</code></pre>\n<p>BeanPostProcessor的使用案例请查看<a href="https://github.com/chkui/spring-core-example" rel="nofollow">实例代码</a>中&nbsp;chkui.springcore.example.xml.beanpostprocessor\n    包中的代码，包含：</p>\n<p>一个实体类：<em>chkui.springcore.example.xml.entity.User</em></p>\n<p>一个服务接口和服务类：<em>chkui.springcore.example.xml.service.UserService</em></p>\n<p>处理器：<em>chkui.springcore.example.xml.beanpostprocessor.Processor</em></p>\n<p>Main入口：<em>chkui.springcore.example.xml.beanpostprocessor.BeanPostProcessor</em></p>\n<p>配置文件：<em>/src/main/resources/xml/config.xml</em></p>\n\n<h3 id="h3-3">更多的后置处理器说明</h3>\n<p>见：<a href="https://www.chkui.com/article/spring/spring_core_post_processor_of_official" title="spring后置处理器">https://www.chkui.com/article/spring/spring_core_post_processor_of_official</a>\n</p>'},404:function(a,s,t){"use strict";Object.defineProperty(s,"__esModule",{value:!0});s.content='<h2 id="h2-1">条件概率</h2>\n<p>事物A独立发生的概率为<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(A)">，事物B独立发生的概率为<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(B)">，那么有：</p>\n<p><img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(A%7CB)">表示事物B发生之后事物A发生的概率；</p>\n<p><img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(B%7CA)">表示事物A发生之后事物B发生的概率；</p>\n\n<h2 id="h2-2">全概率</h2>\n<p>我们可以将公式写成全量的形式：</p>\n<p><img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=B_k(k%3D1%2C2%2C3...%2Cn)">表示全量相互排斥且性质关联的事物，即：\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=B_i%5Ccap%20B_j%3D%5Coslash%20(%E7%A9%BA%E9%9B%86)">，<img\n        alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=B_1%5Ccup%20B_2%20%5Ccup%20.....B_i%20%3D%20%5COmega%20(%E5%85%A8%E9%9B%86%E7%9A%84%E5%AD%90%E9%9B%86)">\n</p>\n<p>那么可以得到</p>\n<p><img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(A)%3D%5Csum_j%5EiP(B_j)P(A%7CB_j)">,这就是全概率公式。\n</p>\n<p>全概率公式的意义在于：无法知道一个事物独立发生的概率，但是我们可以将其在各种条件下发生的概率进行累加获得。</p>\n\n<h3 id="h3-1">全概率的例子</h3>\n<p>\n    例1，已知某种疾病的发病率是0.001，即1000人中会有1个人得病。现有一种试剂可以检验患者是否得病，它的准确率是0.99，即在患者确实得病的情况下，它有99%的可能呈现阳性。它的误报率是5%，即在患者没有得病的情况下，它有5%的可能呈现阳性。一个人检测为阳性的概率是多少。</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E8%AE%BEP(A)%3D0.001%E8%A1%A8%E7%A4%BA%E5%8F%91%E7%97%85%E7%8E%87%EF%BC%8C%E5%88%99P(%5Cbar%7BA%7D%20)%3D0.999%E8%A1%A8%E7%A4%BA%E4%B8%8D%E5%8F%91%E7%97%85%E7%8E%87%E3%80%82P(B)%E8%A1%A8%E7%A4%BA%E6%A3%80%E6%B5%8B%E4%B8%BA%E9%98%B3%E6%80%A7%E7%9A%84%E6%A6%82%E7%8E%87%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%9A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(B)%3DP(A)P(B%7CA)%20%2B%20P(%5Cbar%7BA%7D)P(B%7C%5Cbar%7BA%7D)%EF%BC%8C%E4%B8%94P(B%7CA)%3D0.99%2CP(B%7C%5Cbar%7BA%7D)%3D0.05%2C%E6%89%80%E4%BB%A5">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(B)%3D0.001%C3%970.99%20%2B%200.999%C3%970.05%3D0.05094"></p>\n<p>例2，袋子中50个球，20个黄球，30个白球。2个人一次从袋中各获取一个球，且不放回，求第二个人取得黄球的概率。</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A)%3D%5Cfrac%7B2%7D%7B5%7D%E8%A1%A8%E7%A4%BA%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%8F%96%E5%BE%97%E9%BB%84%E7%90%83%E7%9A%84%E6%A6%82%E7%8E%87%2C%E5%88%99P(%5Cbar%7BA%7D)%3D%5Cfrac%7B3%7D%7B5%7D%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=B%E8%A1%A8%E7%A4%BA%E7%AC%AC%E4%BA%8C%E4%B8%AA%E4%BA%BA%E5%8F%96%E5%BE%97%E9%BB%84%E7%90%83%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%82%E6%9C%89%EF%BC%9A%0A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(B)%3DP(A)P(B%7CA)%2BP(%5Cbar%7BA%7D)P(B%7C%5Cbar%7BA%7D)%3D%5Cfrac%7B2%7D%7B5%7D%C3%97%5Cfrac%7B19%7D%7B49%7D%2B%5Cfrac%7B3%7D%7B5%7D%C3%97%5Cfrac%7B20%7D%7B49%7D%3D%5Cfrac%7B2%7D%7B5%7D">\n</p>\n<p>从另外一个角度说，无论前面的人抽了多少次，后面的人抽签总体概率是不变的。</p>\n<p>例3，5张卡片上分别标记了1,2,3,4,5，每次取2张，连续取2次，取出后不放回。求第二次取出的卡片，比第一次取出的卡片大的概率。</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=A%E8%A1%A8%E7%A4%BA%E7%AC%AC%E4%BA%8C%E5%BC%A0%E7%89%8C%E5%A4%A7%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%82B_i%2Ci%5Cin%5B1%2C5%5D%E8%A1%A8%E7%A4%BA%E7%AC%AC%E4%B8%80%E5%BC%A0%E6%8A%BD%E5%88%B01%E5%88%B05%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E7%AC%AC%E4%B8%80%E5%BC%A0%E6%8A%BD%E5%88%B0%E4%BB%BB%E4%BD%95%E4%B8%80%E5%BC%A0%E7%89%8C%E6%A6%82%E7%8E%87%E9%83%BD%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84%EF%BC%8C%E6%89%80%E4%BB%A5P(B_i)%3D%5Cfrac%7B1%7D%7B5%7D%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E7%AC%AC%E4%B8%80%E5%BC%A0%E7%89%8C%E6%8A%BD%E5%88%B01%E6%97%B6%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%BC%A0%E7%89%8C%E5%A4%A7%E7%9A%84%E6%A6%82%E7%8E%87%E4%B8%BAP(A%7CB_1)%3D1%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E7%AC%AC%E4%B8%80%E5%BC%A0%E7%89%8C%E6%8A%BD%E5%88%B02%E6%97%B6%EF%BC%8C%E7%AC%AC%E4%BA%8C%E5%BC%A0%E7%89%8C%E5%A4%A7%E7%9A%84%E6%A6%82%E7%8E%87%E4%B8%BAP(A%7CB_2)%3D%5Cfrac%7B3%7D%7B4%7D%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E4%BB%A5%E6%AD%A4%E7%B1%BB%E6%8E%A8P(A%7CB_3)%3D%5Cfrac%7B2%7D%7B4%7D%2CP(A%7CB_4)%3D%5Cfrac%7B1%7D%7B4%7D%2CP(A%7CB_5)%3D0%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%9A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A)%3D%5Csum_%7Bi%3D1%7D%5E%7B5%7DP(B_i)P(A%7CB_i)%3D%5Cfrac%7B1%7D%7B5%7D%C3%971%2B%5Cfrac%7B1%7D%7B5%7D%C3%97%5Cfrac%7B3%7D%7B4%7D%2B%5Cfrac%7B1%7D%7B5%7D%C3%97%5Cfrac%7B2%7D%7B4%7D%2B%5Cfrac%7B1%7D%7B5%7D%C3%97%5Cfrac%7B1%7D%7B4%7D%2B%5Cfrac%7B1%7D%7B5%7D%C3%970%3D%5Cfrac%7B1%7D%7B2%7D">\n</p>\n<p>例4，甲袋有5只白球、7个红球，乙袋有4只白球、2只红球。任意取一个袋子，求从袋子取得白球的概率。</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E8%AE%BEA%3A%E8%8E%B7%E5%8F%96%E7%9A%84%E7%99%BD%E7%90%83%E7%9A%84%E4%BA%8B%E4%BB%B6%EF%BC%8CB%3A%E8%8E%B7%E5%BE%97%E7%94%B2%E8%A2%8B%E5%AD%90%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%81%5Cbar%7BB%7D%3A%E8%8E%B7%E5%8F%96%E4%B9%99%E8%A2%8B%E5%AD%90%E3%80%82%E9%82%A3%E4%B9%88%EF%BC%9A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A)%3DP(B)P(A%7CB)%2BP(%5Cbar%7BB%7D)P(A%7C%5Cbar%7BB%7D)"></p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(B)%3DP(%5Cbar%7BB%7D)%3D%5Cfrac%7B1%7D%7B2%7D%2CP(A%7CB)%3D%5Cfrac%7B5%7D%7B12%7D%2CP(A%7C%5Cbar%7BB%7D)%3D%5Cfrac%7B4%7D%7B6%7D%2C%E6%89%80%E4%BB%A5%EF%BC%9A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(B)%3D%5Cfrac%7B1%7D%7B2%7D%C3%97%5Cfrac%7B5%7D%7B12%7D%2B%5Cfrac%7B1%7D%7B2%7D%C3%97%5Cfrac%7B4%7D%7B6%7D%3D%5Cfrac%7B13%7D%7B24%7D">\n</p>\n\n<h2 id="h2-3">*贝叶斯公式</h2>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CB)%3DP(A)%C3%97%5Cfrac%7BP(B%7CA)%7D%7BP(B)%7D"></p>\n<p><strong>贝叶斯公式的理解</strong>：</p>\n<p>可以理解他是全概率公式的反向应用，他是求某个条件出现时某个事件发生的概率。定义如下：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A)%E8%A1%A8%E7%A4%BA%E5%89%8D%E7%BD%AE%E6%A6%82%E7%8E%87%EF%BC%8C%E8%A1%A8%E7%A4%BA%E5%BD%93B%E4%BA%8B%E4%BB%B6%E6%9C%AA%E5%8F%91%E7%94%9F%E6%97%B6A%E4%BA%8B%E4%BB%B6%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CB)%E4%B8%BA%E5%90%8E%E7%BD%AE%E6%A6%82%E7%8E%87%EF%BC%8C%E8%A1%A8%E7%A4%BAB%E4%BA%8B%E4%BB%B6%E5%8F%91%E7%94%9F%E4%B9%8B%E5%90%8EA%E4%BA%8B%E4%BB%B6%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87%E3%80%82">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%81%9A%E6%98%AF%E4%BA%8B%E4%BB%B6B%E5%8F%91%E7%94%9F%E5%90%8E%E5%AF%B9%E5%89%8D%E7%BD%AE%E6%A6%82%E7%8E%87%E7%9A%84%E4%BF%AE%E6%AD%A3%EF%BC%8C%5Cfrac%7BP(B%7CA)%7D%7BP(B)%7D%E6%98%AF%E4%BF%AE%E6%AD%A3%E5%9B%A0%E5%AD%90%E3%80%82">\n</p>\n<p>沿用前面医学的例子：</p>\n<p><em>例1，已知某种疾病的发病率是0.001，即1000人中会有1个人得病。现有一种试剂可以检验患者是否得病，它的准确率是0.99，即在患者确实得病的情况下，它有99%的可能呈现阳性。它的误报率是5%，即在患者没有得病的情况下，它有5%的可能呈现阳性。一个人检测为阳性时候，他确切患病的几率是多少。</em>\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E8%AE%BEP(A)%3D0.001%E8%A1%A8%E7%A4%BA%E5%8F%91%E7%97%85%E7%8E%87%EF%BC%8C%E5%88%99P(%5Cbar%7BA%7D%20)%3D0.999%E8%A1%A8%E7%A4%BA%E4%B8%8D%E5%8F%91%E7%97%85%E7%8E%87%E3%80%82P(B%7CA)%3D0.99%2CP(B%7C%5Cbar%7BA%7D)%3D0.05%E3%80%82%E6%89%80%E4%BB%A5%EF%BC%9A"><img\n        alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CB)%3DP(A)%C3%97%5Cfrac%7BP(B%7CA)%7D%7BP(B)%7D%2C%E7%94%A8%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F%EF%BC%9A">\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CB)%3DP(A)%C3%97%5Cfrac%7BP(B%7CA)%7D%7BP(A)P(B%7CA)%2BP(%5Cbar%7BA%7D)P(B%7C%5Cbar%7BA%7D)%7D"><img\n        alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CB)%3D0.001%C3%97%5Cfrac%7B0.99%7D%7B0.001%C3%970.99%2B0.999*0.05%7D%5Capprox%200.01943%3D1.94%5C%25">\n</p>\n<p>从结论看，这个试剂挺不可靠的。</p>\n<p>将贝叶斯公式的底部展开为全概率公式：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A_k%7CB)%3DP(A_k)%C3%97%5Cfrac%7BP(B%7CA_k)%7D%7B%5Csum_%7Bj%3D1%7D%5EnP(A_j)P(B%7CA_j)%7D%2Cj%5Cin(0%2Cn)%2CA_j%E8%A1%A8%E7%A4%BA%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B%E7%9A%84%E4%BA%8B%E4%BB%B6%E3%80%82">\n</p>\n<p><strong>使用全概率公式展开之后有个很直观的发现：</strong>当我们考察某一个事件的条件概率时——事件<img alt="机器学习中的数学——概率与统计"\n                                                                 src="https://math.jianshu.com/math?formula=B">发生之后<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=A_k">发生的概率，需要将整个样本空间中其他概率事件也加入到其中来。</p>\n\n<h2 id="h2-4">似然函数</h2>\n<p>似然函数个人理解是一种更加“公式化”的条件概率表达式，因为他书写的形式和条件概率相比并没有太大区别——<img alt="机器学习中的数学——概率与统计"\n                                                           src="https://math.jianshu.com/math?formula=P(x%7C%5Ctheta%20)">,只是解读方式不同。这里的<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=x">表示样本特征数据，<img alt="机器学习中的数学——概率与统计"\n                                                                                          src="https://math.jianshu.com/math?formula=%5Ctheta%20">表示模型参数。\n</p>\n<p>如果<img alt="机器学习中的数学——概率与统计"\n          src="https://math.jianshu.com/math?formula=%5Ctheta%20">已知并且固定，那么表示这个是一个概率计算模型，表示：不同的样本<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=x">在固定的模型参数<img alt="机器学习中的数学——概率与统计"\n                                                                                         src="https://math.jianshu.com/math?formula=%5Ctheta%0A">的概率值。\n</p>\n<p>如果<img alt="机器学习中的数学——概率与统计"\n          src="https://math.jianshu.com/math?formula=x">已经并且固定，表示这是一个似然计算模型（统计模型），表示不同的样本用于求解模型参数<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta">。</p>\n\n<h2 id="h2-5">极大似然估计</h2>\n<p>按照前面似然函数<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(x%7C%5Ctheta)">的介绍，似然函数可以看做<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=x">是已知的，<img alt="机器学习中的数学——概率与统计"\n                                                                                      src="https://math.jianshu.com/math?formula=%5Ctheta">是未知的，极大似然估计就是在已知<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=x">的情况下求取<img alt="机器学习中的数学——概率与统计"\n                                                                                       src="https://math.jianshu.com/math?formula=%5Ctheta%20">。\n</p>\n<p>在现实的生产生活中也常常会遇到这样的问题。我们以及有了<strong>样本</strong>以及对应的<strong>标签（结论）</strong>，如何根据这些样本来计算（推算）条件<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta%20">是一件很困难的事情。而极大似然估计就是一个根据样本值<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=x">和结论数据<img alt="机器学习中的数学——概率与统计"\n                                                                                      src="https://math.jianshu.com/math?formula=P(x%7C%5Ctheta)">计算条件参数<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta%20">的过程。</p>\n<p>总的来说，极大似然估计是一种<strong>参数估计算法</strong>。使用极大似然估计有一个很重要的先决条件——每<strong>一组样本都是独立的，并且有充分的训练样本</strong>。</p>\n<p>先看看样本独立的判断公式：<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(A%2CB)%3DP(A)%C3%97P(B)">，即2个事物同时发生的概率等于事物独立发生概率的乘积。\n</p>\n<p>极大似然评估的公式及像这个公式。</p>\n<p>设有一组样本<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=D%3D%5C%7Bx_1%2Cx_2%2Cx_3...x_n%5C%7D">,所有样本的联合概率密度<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P(D%7C%5Ctheta)">称为相对于样本<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5C%7Bx_1%2Cx_2%2Cx_3...x_n%5C%7D">的似然函数。那么由独立判定公式推断出所有样本的概率为：\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=l(%5Ctheta)%3DP(D%7C%5Ctheta)%3DP(x_i%7C%5Ctheta)%3D%5Cprod_%7Bi%3D1%7D%5En%20P(x_i%7C%5Ctheta)">。\n</p>\n<p>设<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D">是使得<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=l(%5Ctheta)">取得最大值的<img alt="机器学习中的数学——概率与统计"\n                                                                                                 src="https://math.jianshu.com/math?formula=%5Ctheta%20">值，那么<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D%20">是<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta%20">的极大似然估计量。可以使用下面的公式表示<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D%20">与<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=D">的关系：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%5Chat%7B%5Ctheta%7D%3Dd(D)%3DD%5C%7Bx_1%2Cx_2%2Cx_3...x_n%5C%7D">,<img\n        alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(x%7C%5Chat%7B%5Ctheta%7D)%E7%A7%B0%E4%B8%BA%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E8%AF%84%E4%BC%B0%E5%80%BC%E3%80%82">\n</p>\n<p>实际计算时，计算连乘比较麻烦，我们可以引入对数将其转换为一个求和的过程：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=L(%5Ctheta)%3Dlnl(%5Ctheta)%3D%5Csum_i%5EnlnP(x_i%7C%5Ctheta)">,因为<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=lnxy%3Dlnx%2Blny">。<img alt="机器学习中的数学——概率与统计"\n                                                                                                 src="https://math.jianshu.com/math?formula=L(%5Ctheta)">也称为对数似然函数。\n</p>\n<p>如果<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=L(%5Ctheta)">连续可微，那么可以使用导数为0求函数的凸点。即：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%5Cfrac%7Bd(L(%5Ctheta))%7D%7Bd%5Ctheta%7D%20%3D%200">。</p>\n<p>将条件因子扩展为M个，即<img alt="机器学习中的数学——概率与统计"\n                    src="https://math.jianshu.com/math?formula=P(x_i%2C%5Ctheta_j)%2Ci%5Cin(0%2Cn%5D%2Cj%5Cin(0%2Cm%5D">,则似然函数（对数似然函数变成）：\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=L(%5Ctheta_j)%3D%5Csum_%7Bi%3D1%7D%5En%5Cln%20P(x_i%7C%5Ctheta_j)">\n</p>\n<p>此时每一个<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta_j">的求导变成一个求偏导数的过程：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%5Cfrac%7B%E2%88%82L(%5Ctheta_j)%7D%7B%E2%88%82%5Ctheta_k%7D%20%3D%20%5Cfrac%7B%E2%88%82%5Csum_%7Bi%3D1%7D%5En%5Cln%20P(x_i%7C%5Ctheta_j)%7D%7B%E2%88%82%5Ctheta_k%7D">,每一个<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=%5Ctheta_j">都要对<img alt="机器学习中的数学——概率与统计"\n                                                                                             src="https://math.jianshu.com/math?formula=L(%5Ctheta_j)">求导。\n</p>\n\n<h3 id="h3-2">最大似然评估的案例</h3>\n\n<h4 id="h4-1">最大似然评估计算</h4>\n<p>\n    最大似然评估（也称为极大似然评估）的用处是什么？首先可以将每个字眼拆解开来看。<strong>最大</strong>就是要找最大值<strong>，似然</strong>说明并不精确似乎就是这个值<strong>，评估</strong>指的是这是一个过程。\n</p>\n<p>现实生活中的例子：2对夫妇<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=(A%2C%5Chat%7BA%7D)%20">和<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=(B%2C%5Chat%7BB%7D)%20">和一个小孩<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=C">。从外观上看，小孩<img alt="机器学习中的数学——概率与统计"\n                                                                                          src="https://math.jianshu.com/math?formula=C">长相比较接近夫妇<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=(A%2C%5Chat%7BA%7D)%20">，有点像<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=B">，不像<img alt="机器学习中的数学——概率与统计"\n                                                                                    src="https://math.jianshu.com/math?formula=%5Chat%7BB%7D%20">,让你猜测<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=C">是谁的小孩。思维正常一点的人肯定会说<img alt="机器学习中的数学——概率与统计"\n                                                                                                   src="https://math.jianshu.com/math?formula=C">是<img\n        alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=(A%2C%5Chat%7BA%7D)">的小孩，这本身就是一个自然而然的判断过程，用数学解释：\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=%E8%AE%BEP(A%7CC)%3EP(B%7CC)%EF%BC%8CP(%5Chat%7BA%7D%7CC)%3EP(%5Chat%7BB%7D%7CC)%E3%80%82P(x%7CC)%E8%A1%A8%E7%A4%BA%E5%B0%8F%E5%AD%A9%E6%9B%B4%E5%83%8F%E8%B0%81%EF%BC%8Cx%5Cin%5BA%2C%5Chat%7BA%7D%2CB%2C%5Chat%7BB%7D%5D%E3%80%82">\n</p>\n<p>使用似然评估，就可以断定小孩更像谁：</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=P(A%7CC)%C3%97P(%5Chat%7BA%7D%7CC)%20%3E%20P(B%7CC)%C3%97P(%5Chat%7BB%7D%7CC)">。\n</p>\n\n<h4 id="h4-2">参数&amp;模型评估</h4>\n<p>最大似然估计更多的应用是在有一定样本数据的情况下用于模型评估，更准确的说是模型中的参数评估。因为似然评估来自于概率独立判决公式——<img alt="机器学习中的数学——概率与统计"\n                                                                         src="https://math.jianshu.com/math?formula=P(A%2CB)%3DP(A)%C3%97P(B)">,所以要求用于评估的样本数据相互独立。\n</p>\n<p>先说一个很直观的案例解释这个问题：</p>\n<p><em>例1，从盒子里连续取球，已知取得红球的概率<img alt="机器学习中的数学——概率与统计"\n                                 src="https://math.jianshu.com/math?formula=P%5Cin%5B0.1%2C0.5%5D">,求当P取何值时最有可能连续三次拿到红球。</em>\n</p>\n<p>只管上来说，肯定是概率越高取得红球的几率越高，所以不做推断也知道<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=P%3D0.5">时拿到红球的几率更高。下面通过数学过程来说明这个问题。\n</p>\n<p>设条件<img alt="机器学习中的数学——概率与统计" src="https://math.jianshu.com/math?formula=Y_i%3D1">，表示取得红球，<img alt="机器学习中的数学——概率与统计"\n                                                                                                  src="https://math.jianshu.com/math?formula=Y_i%3D0">表示没取得红球，所以用最大似然评估来计算参数得：\n</p>\n<p><img alt="机器学习中的数学——概率与统计"\n        src="https://math.jianshu.com/math?formula=l(%5Ctheta)%3DP(Y_1)%C3%97P(Y_2)%C3%97P(Y_3)%3DP%5E3">，只管的看就知道取值0.5似然评估最大。\n</p>'},405:function(a,s,t){"use strict";Object.defineProperty(s,"__esModule",{value:!0});s.content='<h2 id="h2-1">概念与应用</h2>\n<p><strong>Softmax</strong>是机器学习中一个非常重要的工具，他可以兼容 logistics 算法、可以独立作为机器学习的模型进行建模训练、还可以作为深度学习的激励函数。<br>\n    <strong>softmax</strong>的作用简单的说就计算一组数值中每个值的占比，公式一般性描述为：<br> 设一共有<img alt="机器学习——softmax计算"\n                                                                         src="https://math.jianshu.com/math?formula=n">个用数值表示的分类<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=S_k%EF%BC%8Ck%5Cin(0%2Cn%5D">，其中<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=n">表示分类的个数。那么softmax计算公式为：<br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=P(S_i)%3D%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_k%5Ene%5E%7Bg_k%7D%7D%2Ci%E8%A1%A8%E7%A4%BAk%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%AA%E5%88%86%E7%B1%BB%EF%BC%8Cg_i%E8%A1%A8%E7%A4%BA%E8%AF%A5%E5%88%86%E7%B1%BB%E7%9A%84%E5%80%BC"\n            class="zoom-in-cursor">。</p>\n<p>在机器学习中经常用它来解决MECE原则的分类——每一个分类相互独立，所有的分类被完全穷尽。比如男人和女人就是负责MECE原则的。</p>\n\n<h2 id="h2-2">softmax的例子</h2>\n<p>看一个例子能更好的理解<strong>softmax</strong>。<br> 设有三个数值<img alt="机器学习——softmax计算"\n                                                       src="https://math.jianshu.com/math?formula=A%3D5%2CB%3D1%2CC%3D-1">，那么他们的softmax占比为：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=P(A)%3D%5Cfrac%7Be%5E5%7D%7Be%5E5%2Be%2Be%5E%7B-1%7D%7D"><br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=P(B)%3D%5Cfrac%7Be%7D%7Be%5E5%2Be%2Be%5E%7B-1%7D%7D"><br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=P(C)%3D%5Cfrac%7Be%5E%7B-1%7D%7D%7Be%5E5%2Be%2Be%5E%7B-1%7D%7D"><br>\n    计算结果为：<br> <img alt="机器学习——softmax计算"\n                    src="https://math.jianshu.com/math?formula=P(A)%3D0.9817%2CP(B)%3D0.0180%2CP(C)%3D0.0003"\n                    class="zoom-in-cursor"><br> <img alt="机器学习——softmax计算"\n                                                     src="https://math.jianshu.com/math?formula=P(A)%2BP(B)%2BP(C)%3D1"\n                                                     class="zoom-in-cursor"></p>\n\n<h2 id="h2-3">基本特性</h2>\n<p>从上面的计算结果可以看出<em>softmax</em>的一些特性：</p>\n<ol>\n    <li>归一化：最后的合计为1，每一个分类都是一个小于1的数值。</li>\n    <li>放大效果：上面的例子中单纯从数值来看，5和1的差距并不大，但是通过指数运算有明显的放大效果，5的占比能到98%以上。</li>\n    <li>散列性质，每一个比率虽然最后都会进行归一，但是他们放大之前的数值是可以相互不干扰的。</li>\n</ol>\n\n<h2 id="h2-4">softmax的损失函数</h2>\n<p>softmax的损失函数可以用交叉熵来表述，也可以用极大似然评估来描述，后续的数学推导结论会发现2个算法的结果都是一样的。</p>\n\n<h3 id="h3-1">熵与交叉熵</h3>\n\n<h4 id="h4-1">熵</h4>\n<p>这里所说的熵来源于信息论，他表示“为了确保完整的信息被描述所需要的编码长度”。看起来是一个很拗口的概念，下面看一个例子。</p>\n<p>假设26个英文字母每个字母出现概率都是相同的<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p">（即<img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7B26%7D">）,那么记录26个英文字母所需要的信息量是<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=log_x%5Cfrac%7B1%7D%7Bp%7D">，这个公式就是表述26个字符的熵。如果取<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=x%3D2">表示用一个信息位表示2个信息（也就是我们用来衡量数据大小最小计算单位bit：0/1），那么计算出<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=log_226%3D4.7004%5Capprox5">说明表述所有的英文字符需要5bit的信息量。\n</p>\n\n<h4 id="h4-2">交叉熵</h4>\n<p>在实际使用中大部分事物都不是均匀分布的，比如一篇英文文章中\'e\'出现出现的频率明显多于其他字符，而且有时也无法知道真实分布的情况。这时计算信息量就可以使用交叉熵，它是在非均匀分布下信息量的一种表述表示：<br> <img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=H(p%2Cg)%3D%5Csum_i%5ENp_i%5Clog_x%5Cfrac%7B1%7D%7Bq_i%7D">。这里<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_j">表示每一个事物的真实概率，<img alt="机器学习——softmax计算"\n                                                                                                src="https://math.jianshu.com/math?formula=q_j">表示对应的预估概率。<br>\n    关于交叉熵的详细说明可以看<a href="https://www.chkui.com/article/tensorflow/tensorflow_get_started_of_mnist" target="_blank"\n                    rel="nofollow">本人这篇MNIST介绍的文章关于熵与交叉熵的解释说明</a>。</p>\n\n<h3 id="h3-2">极大似然评估</h3>\n<p>softmax算法可以看做是一个概率问题，设<img alt="机器学习——softmax计算"\n                              src="https://math.jianshu.com/math?formula=A_i">表示不同的分类，每个分类的概率表示为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=P(A_i%7Cw_%7Bij%7Dx_j)">，其中<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=i%20%5Cin%20(0%2CN%5D">表示分类的个数。<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=j%5Cin(0.M%5D">&nbsp;<img alt="机器学习——softmax计算"\n                                                                                                   src="https://math.jianshu.com/math?formula=x_j">表示特征数。设<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_i%3DP(A_i%7Cw_%7Bij%7Dx_j))">，那么在softmax中<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Csum_i%5EN%20q_i%20%3D%201">。用<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_i">表示分类的真实分布，由于事物分类遵守MECE原则，所以所有的<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_i">组合在一起实际上是个由1和0组成的数组，只有一个元素为1值。可以参照logistics回归算法：<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=P%3Dq%5Ep(1-q)%5E%7B(1-p)%7D">，softmax也可以使用类似的结构：<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=P%3D%5Cprod_i%5EN%20q_i%5E%7Bp_i%7D">&nbsp;。用对数最大似然评估作为损失函数：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=L%3D%5Clog%5Cprod_i%5ENq_i%5E%7Bp_i%7D%3D%5Csum_i%5ENp_i%5Clog%7Bq_i%7D"\n         class="zoom-in-cursor">，<br> 可以看出极大似然评估和交叉熵最后得到的是一模一样的表达式。<br> 将公式扩展为<img alt="机器学习——softmax计算"\n                                                                                   src="https://math.jianshu.com/math?formula=M">个样本的情况：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=L%3D%5Cfrac%7B1%7D%7BM%7D%5Clog%5Cprod_k%5EM%5Cprod_i%5ENq_%7Bij%7D%5E%7Bp_%7Bij%7D%7D%3D%5Cfrac%7B1%7D%7BM%7D%5Csum_k%5EM%5Csum_i%5EN%20p_%7Bkj%7D%5Clog%20q_%7Bkj%7D%2C%20k%5Cin(0%2CM%5D"\n         class="zoom-in-cursor"></p>\n\n<h2 id="h2-5">损失函数的含义</h2>\n<p>前面已经提到softmax分类应该遵守MECE原则，所以一个样本属于某个分类会用<strong>“占位”</strong>的方法来标注。例如现在有三个分类，样本A属于第二个分类表示为[0,1,0]、样本B属于第三个分类表示为[0,0,1]、C属于第一个分类——[1,0,0]。每个数组可以看做是的样本分类的真实概率分布——属于某个分类该分类对应的概率就是1，其他分类概率是0。<br>\n    特征和权重参数通过<em>softmax</em>计算之后得到的是一个概率分布。假设样本A的特征通过softmax计算后分类的概率是[0.2,0.6,0.2]，这个时候对于损失函数的计算结果是：<img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=0%C3%97%5Cln%200.2%2B1%C3%97%5Cln%200.6%20%2B%200%20%C3%97%20%5Cln%200.2%20%3D%20%5Cln%200.6%20%5Capprox%20-0.51"\n            class="zoom-in-cursor">。<br> 我们放大真实分布的比重为[0.1,0.8,0.1]后，计算结果：<img alt="机器学习——softmax计算"\n                                                                              src="https://math.jianshu.com/math?formula=%5Cln%200.8%20%5Capprox%20-0.22">，放大到[0.05,0.9,0.05]得：<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Capprox%20-0.10">。所以一个很直观的感受是：损失函数是从负数无限接近0。\n</p>\n<p>下面通过大量的数据来模拟这个过程。假设所有的样本属于2个分类，样本分类的标注固定为[1,0]，随机生成100个样本模拟分类的概率为：<br> <img alt="机器学习——softmax计算"\n                                                                               src="https://math.jianshu.com/math?formula=%5Cbegin%7Bmatrix%7D%20p_1%26p_2%5C%5C%200.2%260.8%20%5C%5C%200.7%260.3%20%5C%5C%200.9%260.1%20%5C%5C%20%E2%80%A6%26%E2%80%A6%20%5Cend%7Bmatrix%7D"><br>\n    那么这100组数据和损失函数计算结果构成的关系如下图：</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://upload-images.jianshu.io/upload_images/2418406-e1dfac238be8f1a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"\n        class="zoom-in-cursor"></p>\n<p>交叉熵与分类的概率的关系</p>\n<p><br> 由于所有样本的标注都是[1,0]，所以<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_1">的概率越接近1、<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_2">越接近0越符合真实分布。可以看到当<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_1">接近1<img alt="机器学习——softmax计算"\n                                                                                      src="https://math.jianshu.com/math?formula=q_2">接近0时，交叉熵的计算结果从负数方向接近0。可以执行<a\n        href="https://github.com/chkui/ml-math-softmax/blob/master/sample/matplot/corss_entropy_feature.py"\n        target="_blank" rel="nofollow">模拟过程的源码</a>用matplotlib看到更清晰的结果。</p>\n<p></p>\n<p>再使用一个过程来确认这个结果。<strong>softmax</strong>是体现一组数值的占比，被标记的那个分类占比越高越接近真实分布。现在假设有5000组样本，每个样本对应20个分类，每个分类的特征值在0～10之间随机产生，每个样本的标记在0~20之间随机设定。现在看看标记项的概率值与损失函数的关系：\n</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://upload-images.jianshu.io/upload_images/2418406-832b3f8fdcfba154.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"\n        class="zoom-in-cursor"></p>\n<p>标注项占比与交叉熵关系趋势</p>\n<p><br> 图中<em>softmax highest</em>表示标注项的概率（占比），<em>corss entropy</em>就是损失函数的计算结果。可以看到当标记项概率越接近1，损失的计算结果越接近0。如果有兴趣可以<a\n        href="https://github.com/chkui/ml-math-softmax/blob/master/sample/matplot/corss_entropy_ratio.py"\n        target="_blank" rel="nofollow">使用生成图像的代码</a>了解分析过程。</p>\n<p></p>\n\n<h2 id="h2-6">建模</h2>\n\n<h3 id="h3-3">softmax计算</h3>\n<p>上面的内容介绍了softmax的公式以及损失函数。下面说明其如何运算。<br> 在实际应用中一个样本的特征是一个的向量：<img alt="机器学习——softmax计算"\n                                                                    src="https://math.jianshu.com/math?formula=X%3D%7Bx_1%2Cx_2%2Cx_3%2C....x_n%7D">，每一个特征在计算过程中都有一个权重，所以引入权重参数建立权重结构（直线结构）：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=g(x)%3Dw_0%2Bw_1x_1%2Bw_2x_2%2B...%2Bw_nx_n%3Dw_jx_j">，<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=j%5Cin%5B0%2Cn%5D%EF%BC%8Cx_0%3D1"><br>\n    所以softmax更加完整的代数表达式是：<br> <img alt="机器学习——softmax计算"\n                                   src="https://math.jianshu.com/math?formula=softmax(x)%3D%5Cfrac%7Be%5E%7Bw_%7Bij%7Dx_j%7D%7D%7B%5Csum_j%5Ene%5E%7Bw_ijx_j%7D%7D"><br>\n    其中<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=i">表示计算结果有多少个分类<img alt="机器学习——softmax计算"\n                                                                                                src="https://math.jianshu.com/math?formula=i%5Cin(0%2Cm)">，j表示特征的个数<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=j%5Cin%5B0.n%5D">。<br> 有<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=o">个样本时就扩展为一个2阶张量，那么用矩阵形式表述更加简洁：<br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=softmax(X)%3D%5Cfrac%7Be%5E%7BXW%5ET%7D%7D%7Be%5E%7BXW%5ET%7DE%7D"><br>\n    用下标<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=_%7B(o)%7D">表示当前的特征属于第几个样本，例如<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=x_%7B(1)3%7D">表示第1个样本的第3个特征。矩阵的计算过程如下：</p>\n\n<h4 id="h4-3">1.计算权重指数</h4>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=G%3Dexp%5Cleft(%20%5Cbegin%7Bbmatrix%7D%20x_%7B(1)0%7D%26x_%7B(1)1%7D%26x_%7B(1)2%7D%26%5Ccdots%26x_%7B(1)n%7D%5C%5C%20x_%7B(2)0%7D%26x_%7B(2)1%7D%26x_%7B(2)2%7D%26%5Ccdots%26x_%7B(2)n%7D%5C%5C%20x_%7B(3)0%7D%26x_%7B(3)1%7D%26x_%7B(3)2%7D%26%5Ccdots%26x_%7B(3)n%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20x_%7B(o)0%7D%26x_%7B(o)1%7D%26x_%7B(o)2%7D%26%5Ccdots%26x_%7B(o)n%7D%20%5Cend%7Bbmatrix%7D%20%C3%97%20%5Cbegin%7Bbmatrix%7D%20w_%7B10%7D%26w_%7B20%7D%26w_%7B30%7D%26%5Ccdots%26w_%7Bm0%7D%5C%5C%20w_%7B11%7D%26w_%7B21%7D%26w_%7B31%7D%26%5Ccdots%26w_%7Bm1%7D%5C%5C%20w_%7B12%7D%26w_%7B22%7D%26w_%7B32%7D%26%5Ccdots%26w_%7Bm2%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20w_%7B1n%7D%26w_%7B2n%7D%26w_%7B3n%7D%26%5Ccdots%26w_%7Bmn%7D%5C%5C%20%5Cend%7Bbmatrix%7D%5Cright)"><br>\n    矩阵中<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=x_%7B(o)0%7D%20%3D%201"><br> <img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=exp()">表示矩阵每一个元素求e指数。所以得到：</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=G%3D%20%5Cbegin%7Bbmatrix%7D%20e%5E%7B%5Csum_j%5Enw_%7B1j%7Dx_%7B(1)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B2j%7Dx_%7B(1)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B3j%7Dx_%7B(1)j%7D%7D%26%5Ccdots%26e%5E%7B%5Csum_j%5Enw_%7Bmj%7Dx_%7B(1)j%7D%7D%5C%5C%20e%5E%7B%5Csum_j%5Enw_%7B1j%7Dx_%7B(2)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B2j%7Dx_%7B(2)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B3j%7Dx_%7B(2)j%7D%7D%26%5Ccdots%26e%5E%7B%5Csum_j%5Enw_%7Bmj%7Dx_%7B(2)j%7D%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20e%5E%7B%5Csum_j%5Enw_%7B1j%7Dx_%7B(o)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B2j%7Dx_%7B(o)j%7D%7D%26e%5E%7B%5Csum_j%5Enw_%7B3j%7Dx_%7B(o)j%7D%7D%26%5Ccdots%26e%5E%7B%5Csum_j%5Enw_%7Bmj%7Dx_%7B(o)j%7D%7D%20%5Cend%7Bbmatrix%7D">\n</p>\n<p>令<img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=c_%7Bki%7D%3Dc(w_%7Bij%7D%2Cx_%7B(k)j%7D)%3De%5E%7B%5Csum_j%5Enw_%7Bij%7Dx_%7B(k)j%7D%7D%2Ck%5Cin(0%2Co%5D">，有：\n</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=G%3DG(x)%3D%20%5Cbegin%7Bbmatrix%7D%20c_%7B11%7D%26c_%7B12%7D%26c_%7B13%7D%26%5Ccdots%26c_%7B1m%7D%5C%5C%20c_%7B21%7D%26c_%7B22%7D%26c_%7B23%7D%26%5Ccdots%26c_%7B2m%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20c_%7Bo1%7D%26c_%7Bo2%7D%26c_%7Bo3%7D%26%5Ccdots%26c_%7Bom%7D%5C%5C%20%5Cend%7Bbmatrix%7D">\n</p>\n\n<h4 id="h4-4">2.计算分母</h4>\n<p>现在<img alt="机器学习——softmax计算"\n          src="https://math.jianshu.com/math?formula=Softmax%3DS(x)%3D%5Cfrac%7BG%7D%7BG%C3%97E%7D"><br> <img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=E">是一个形状为<img alt="机器学习——softmax计算"\n                                                                                       src="https://math.jianshu.com/math?formula=m%C3%971">元素全为1的矩阵：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=E%3D%5Cbegin%7Bbmatrix%7D%201%5C%5C%201%5C%5C%20%5Ccdots%5C%5C%201%20%5Cend%7Bbmatrix%7D">\n</p>\n<p>分母:<img alt="机器学习——softmax计算"\n           src="https://math.jianshu.com/math?formula=S%3DG%C3%97E%3D%5Cbegin%7Bbmatrix%7D%20c_%7B11%7D%26c_%7B12%7D%26c_%7B13%7D%26%5Ccdots%26c_%7B1m%7D%5C%5C%20c_%7B21%7D%26c_%7B22%7D%26c_%7B23%7D%26%5Ccdots%26c_%7B2m%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20c_%7Bo1%7D%26c_%7Bo2%7D%26c_%7Bo3%7D%26%5Ccdots%26c_%7Bom%7D%5C%5C%20%5Cend%7Bbmatrix%7D%C3%97%5Cbegin%7Bbmatrix%7D%201%5C%5C%201%5C%5C%20%5Ccdots%5C%5C%201%20%5Cend%7Bbmatrix%7D"><br>\n    所以：<img alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=S%3D%5Cbegin%7Bbmatrix%7D%20%5Csum_i%5Emc_%7B1i%7D%5C%5C%20%5Csum_i%5Emc_%7B2i%7D%5C%5C%20%5Csum_i%5Emc_%7B3i%7D%5C%5C%20%5Ccdots%5C%5C%20%5Csum_i%5Emc_%7Boi%7D%20%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7Ds_1%5C%5Cs_2%5C%5Cs_3%5C%5C%5Ccdots%5C%5Cs_o%5Cend%7Bbmatrix%7D">\n</p>\n\n<h4 id="h4-5">3.归一化</h4>\n<p>现在<img alt="机器学习——softmax计算"\n          src="https://math.jianshu.com/math?formula=Q%3Dsoftmax%3D%5Cfrac%7BG%7D%7BS%7D%3D%5Cbegin%7Bbmatrix%7D%20c_%7B11%7D%26c_%7B12%7D%26c_%7B13%7D%26%5Ccdots%26c_%7B1m%7D%5C%5C%20c_%7B21%7D%26c_%7B22%7D%26c_%7B23%7D%26%5Ccdots%26c_%7B2m%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20c_%7Bo1%7D%26c_%7Bo2%7D%26c_%7Bo3%7D%26%5Ccdots%26c_%7Bom%7D%5C%5C%20%5Cend%7Bbmatrix%7D%5Cdiv%5Cbegin%7Bbmatrix%7Ds_1%5C%5Cs_2%5C%5Cs_3%5C%5C%5Ccdots%5C%5Cs_o%5Cend%7Bbmatrix%7D"><br>\n    所以最终<img alt="机器学习——softmax计算"\n             src="https://math.jianshu.com/math?formula=Q%3D%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7Bc_%7B11%7D%7D%7Bd_1%7D%26%5Cfrac%7Bc_%7B12%7D%7D%7Bd_1%7D%26%5Cfrac%7Bc_%7B13%7D%7D%7Bd_1%7D%26%5Ccdots%26%5Cfrac%7Bc_%7B1m%7D%7D%7Bd_1%7D%5C%5C%20%5Cfrac%7Bc_%7B21%7D%7D%7Bd_2%7D%26%5Cfrac%7Bc_%7B22%7D%7D%7Bd_2%7D%26%5Cfrac%7Bc_%7B23%7D%7D%7Bd_2%7D%26%5Ccdots%26%5Cfrac%7Bc_%7B2m%7D%7D%7Bd_2%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20%5Cfrac%7Bc_%7Bo1%7D%7D%7Bd_o%7D%26%5Cfrac%7Bc_%7B12%7D%7D%7Bd_o%7D%26%5Cfrac%7Bc_%7B12%7D%7D%7Bd_o%7D%26%5Ccdots%26%5Cfrac%7Bc_%7B1m%7D%7D%7Bd_o%7D%5C%5C%20%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7D%20q_%7B11%7D%26q_%7B12%7D%26q_%7B13%7D%26%5Ccdots%26q_%7B1m%7D%5C%5C%20q_%7B21%7D%26q_%7B22%7D%26q_%7B23%7D%26%5Ccdots%26q_%7B2m%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20q_%7Bo1%7D%26s_%7Bo2%7D%26q_%7Bo3%7D%26%5Ccdots%26q_%7Bom%7D%5C%5C%20%5Cend%7Bbmatrix%7D">\n</p>\n\n<h3 id="h3-4">交叉熵（极大似然评估）计算</h3>\n<p>根据交叉熵的公式<img alt="机器学习——softmax计算"\n                src="https://math.jianshu.com/math?formula=L%3D%5Csum_i%5Emp_i%5Clog%20q_i">，这里<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_i">是样本的真实分类（标签label），<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_i">是softmax计算的结果。用矩阵结构表示：<br> <img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=L%3D%5Cfrac%7B1%7D%7Bo%7D%5Cleft%5BP%5Clog%5Cleft(Q%5ET%5Cright)%5Cright%5D%5ED%C3%97E">，矩阵<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=A_%7Bm%C3%97n%7D%5ED">表示取对角线元素形成一个<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=1%C3%97n">的矩阵。</p>\n\n<h4 id="h4-6">1.对数及矩阵乘积</h4>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=%5Cleft%5BP%5Clog%5Cleft(S%5ET%5Cright)%5Cright%5D%5ED%3D%20%5Cleft%5B%5Cbegin%7Bbmatrix%7D%20p_%7B(1)1%7D%26p_%7B(1)2%7D%26p_%7B(1)3%7D%26%5Ccdots%26p_%7B(1)m%7D%5C%5C%20p_%7B(2)1%7D%26p_%7B(2)2%7D%26p_%7B(2)3%7D%26%5Ccdots%26p_%7B(2)m%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20p_%7B(o)1%7D%26p_%7B(o)2%7D%26p_%7B(o)3%7D%26%5Ccdots%26p_%7B(o)m%7D%5C%5C%20%5Cend%7Bbmatrix%7D%C3%97%5Clog%5Cleft(%5Cbegin%7Bbmatrix%7D%20s_%7B11%7D%26s_%7B21%7D%26%5Ccdots%26s_%7Bo1%7D%5C%5C%20s_%7B12%7D%26s_%7B22%7D%26%5Ccdots%26s_%7Bo2%7D%5C%5C%20%5Cvdots%26%5Cvdots%26%5Cddots%26%5Cvdots%5C%5C%20s_%7B1m%7D%26s_%7B2m%7D%26%5Ccdots%26s_%7Bom%7D%5C%5C%20%5Cend%7Bbmatrix%7D%5Cright)%5Cright%5D%5ED"><br>\n    对数<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Clog()">表示对每个元素进行对数运算，他仅改变每个元素的值，对矩阵结构没任何影响，所以下面用<img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=s_%7Bom%7D%E8%A1%A8%E7%A4%BA%5Clog(s_%7Bom%7D)">继续表示：<br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=%5Cleft%5BP%5Clog%5Cleft(S%5ET%5Cright)%5Cright%5D%5ED%3D%20%5Cbegin%7Bbmatrix%7D%20%5Csum_i%5Em(p_%7B(1)i%7Ds_%7B(1)i%7D)%26%20%5Csum_i%5Em(p_%7B(2)i%7Ds_%7B(2)i%7D)%26%20%5Ccdots%26%20%5Csum_i%5Em(p_%7B(o)i%7Ds_%7B(o)i%7D)%20%5Cend%7Bbmatrix%7D">\n</p>\n\n<h4 id="h4-7">2.交叉熵计算</h4>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=L%3D%5Cfrac%7B1%7D%7Bo%7D%C3%97%5Cbegin%7Bbmatrix%7D%20%5Csum_i%5Em(p_%7B(1)i%7Ds_%7B(1)i%7D)%26%20%5Csum_i%5Em(p_%7B(2)i%7Ds_%7B(2)i%7D)%26%20%5Ccdots%26%20%5Csum_i%5Em(p_%7B(o)i%7Ds_%7B(o)i%7D)%20%5Cend%7Bbmatrix%7D%C3%97%5Cbegin%7Bbmatrix%7D%201%5C%5C%201%5C%5C%20%5Ccdots%5C%5C%201%20%5Cend%7Bbmatrix%7D"><br>\n    将<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Clog">符号带入公式得到最终的损失函数矩阵计算结果：<br> <img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=L%3D%5Cfrac%7B1%7D%7Bo%7D%5Cbegin%7Bbmatrix%7D%5Csum_k%5Eo%5Csum_i%5Em%5Cleft%5Bp_%7B(o)i%7D%5Clog(s_%7B(o)i%7D)%20%5Cright%5D%5Cend%7Bbmatrix%7D"><br>\n    把矩阵符号去掉，这里的结果和前面<strong>最大似然评估</strong>推导的结果一致。</p>\n\n<h3 id="h3-5">参数优化</h3>\n<p>通过前文的介绍我们知道,损失函数的目标是获得<strong>“最大值”</strong>，这个最大值的含义是从负无穷方向接近0的一个极限过程。所以经常会看到很多文章会在指标函数前面添加一个负号，如下面这样：<br> <img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=Loss%3D-%5Cfrac%7B1%7D%7Bo%7D%5Csum%5Csum%20p%5Cln%20q"><br>\n    这样就可以把这个过程转变为求<strong>“最小值”</strong>——从正无穷方向接近0，本质并没有多大区别。</p>\n<p>既然这是一个极限过程，自然就可以用积分原理逐渐计算合理的参数。现在的目标是通过导数和找到递增量可以逐步求解<img alt="机器学习——softmax计算"\n                                                             src="https://math.jianshu.com/math?formula=w_%7Bij%7D">值：<br>\n    用<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D">表示<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=w_%7Bij%7D">的偏导函数:<img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D%3D%5Cfrac%7B%5Cpartial%20L(w_%7Bij%7D)%7D%7B%5Cpartial%20w_%7Bij%7D%7D">。<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=w_%7Bij%7D">的更新公式为：<img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=w_%7Bij%7D%3Dw_%7Bij%7D%2B%5Ceta%5Cnabla_%7Bij%7D">。<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Ceta">表示每一步更新的步长。<br>\n    如果损失函数前携带了负号，那么更新公式应该修改为<img alt="机器学习——softmax计算"\n                                 src="https://math.jianshu.com/math?formula=w_%7Bij%7D%3Dw_%7Bij%7D-%5Ceta%5Cnabla_%7Bij%7D">，即越来越小。\n</p>\n\n<h4 id="h4-8">1.求偏导函数</h4>\n<p>目的已经明确，那么接下来就是数学运算了：<br> 设softmax计算结果一共有M个分类，输入模型的一个样本一共有N个特征。<br> <img alt="机器学习——softmax计算"\n                                                                           src="https://math.jianshu.com/math?formula=g_i">表示权重计算的结果，下标<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=i">表示所属的分类，用数组可以表示为：<br> <img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=%5Cbegin%7Bbmatrix%7D%20g_1%3D%5Csum_j%5ENw_%7B1j%7Dx_j%5C%5C%20g_2%3D%5Csum_j%5ENw_%7B2j%7Dx_j%5C%5C%20%5Cvdots%5C%5C%20g_i%3D%5Csum_j%5ENw_%7Bij%7Dx_j%5C%5C%20%5Cvdots%5C%5C%20g_M%3D%5Csum_j%5ENw_%7BMj%7Dx_j%5C%5C%20%5Cend%7Bbmatrix%7D"><br>\n    <img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_i">表示每一个分类softmax计算的结果：<img\n            alt="机器学习——softmax计算"\n            src="https://math.jianshu.com/math?formula=q_i%3D%5Cfrac%7Bg_i%7D%7B%5Csum_k%5EMg_k%7D">,k表示分类迭代求和的下标：用数组表示为：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=%5Cbegin%7Bbmatrix%7D%20q_1%3D%5Cfrac%7Be%5E%7Bg_1%7D%7D%7B%5Csum_k%5EMe%5E%7Bg_k%7D%7D%5C%5C%20q_2%3D%5Cfrac%7Be%5E%7Bg_2%7D%7D%7B%5Csum_k%5EMe%5E%7Bg_k%7D%7D%5C%5C%20%5Cvdots%5C%5C%20q_i%3D%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_k%5EMe%5E%7Bg_k%7D%7D%5C%5C%20%5Cvdots%5C%5C%20q_M%3D%5Cfrac%7Be%5E%7Bg_M%7D%7D%7B%5Csum_k%5EMe%5E%7Bg_k%7D%7D%5C%5C%20%5Cend%7Bbmatrix%7D"><br>\n    Loss是最终的损失函数：<img alt="机器学习——softmax计算"\n                      src="https://math.jianshu.com/math?formula=Loss%3D%5Csum_k%5EM%20L_k%20%3D%20%5Csum_k%5EM%20p_k%5Cln%20q_k">。<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_k">表示每一个softmax分类对应的真实概率，取值0或1。<br>\n    优化参数是不断的调优权重参数，所以把<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=w_%7Bij%7D">看做自变量求导：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D%3D%5Cfrac%7B%5Cpartial%20Loss%7D%7B%5Cpartial%20w_%7Bij%7D%7D"><br>\n    按照前面给出的公式将损失函数的计算分为3步：1）计算权重模型，2）计算softmax，3）计算交叉熵。现在把求导过程分为这3步对<img alt="机器学习——softmax计算"\n                                                                         src="https://math.jianshu.com/math?formula=q_i">、<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=g_i">以及<img alt="机器学习——softmax计算"\n                                                                                         src="https://math.jianshu.com/math?formula=w_%7Bij%7D">复合求导：<br>\n    <img alt="机器学习——softmax计算"\n         src="https://math.jianshu.com/math?formula=%5Cbegin%7Bsplit%7D%20%5Cnabla_%7Bij%7D%26%3D%5Cleft(%5Csum_k%5EM%20p_k%5Cln%20q_k%5Cright)%27%5C%5C%20%26%3D%5Csum_k%5EM%5Cfrac%7Bp_k%7D%7Bq_k%7D%5Cleft(%5Csum_k%5EM%5Cfrac%7Be%5E%7Bg_k%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%20%5Cright)%27%5C%5C%20%26%3D%5Csum_k%5EM%5Cfrac%7Bp_k%7D%7Bq_k%7D%5Cfrac%7B%5Cleft(e%5E%7Bg_k%7D%5Cright)%27%5Csum_l%5EMe%5E%7Bg_l%7D%20-%20e%5E%7Bg_k%7D%5Cleft(%5Csum_l%5EMe%5E%7Bg_l%7D%5Cright)%27%7D%7B%5Cleft(%5Csum_l%5EMe%5E%7Bg_l%7D%5Cright)%5E2%7D%5C%5C%20%26%3D%5Csum_k%5EM%5Cfrac%7Bp_k%7D%7Bq_k%7D%5Cleft%5B%5Cfrac%7B%5Cleft(e%5E%7Bg_k%7D%5Cright)%27%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%20-%20%5Cfrac%7Be%5E%7Bg_k%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cleft(g_k%5Cright)%27%5Cright%5D%20%5Cend%7Bsplit%7D">\n</p>\n<p>计算到这里需要注意一个问题。因为目标是对<img alt="机器学习——softmax计算"\n                            src="https://math.jianshu.com/math?formula=w_%7Bij%7D">求导，所以在求和公式中包含<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=w_%7Bij%7D">的项（即包含<img alt="机器学习——softmax计算"\n                                                                                                src="https://math.jianshu.com/math?formula=g_i">的项）和不包含的项求导的结果是不一样的，所以需要将<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=g_i">项单独拿出来求导。所以有：<br> <img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=%5Cbegin%7Bsplit%7D%20%5Cnabla_%7Bij%7D%26%3D%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cleft%5B%5Cfrac%7B%5Cleft(e%5E%7Bg_i%7D%5Cright)%27%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%20-%20%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cleft(g_k%5Cright)%27%5Cright%5D%20%2B%20%5Csum_%7Bk%20%5Cneq%20i%7D%5EM%5Cfrac%7Bp_k%7D%7Bq_k%7D%5Cleft%5B%5Cfrac%7B%5Cleft(e%5E%7Bg_k%7D%5Cright)%27%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%20-%20%5Cfrac%7Be%5E%7Bg_k%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cfrac%7Be%5E%7Bg_i%7D%7D%7B%5Csum_l%5EMe%5E%7Bg_l%7D%7D%5Cleft(g_k%5Cright)%27%5Cright%5D%5C%5C%20%26%3D%5Cfrac%7Bp_i%7D%7Bq_i%7D%5Cleft(q_i-q_i%5E2%5Cright)%5Cleft(g_k%5Cright)%27-%5Csum_%7Bk%20%5Cneq%20i%7D%5EM%5Cfrac%7Bp_k%7D%7Bq_k%7Dq_kq_i%5Cleft(g_k%5Cright)%27%5C%5C%20%26%3D%5Cleft%5Bp_i-%5Cleft(p_iq_i%2B%5Csum_%7Bk%5Cneq%20i%7D%5EMp_kq_i%5Cright)%5Cright%5D%5Cleft(g_k%5Cright)%27%5C%5C%20%26%3D%5Cleft(p_i-q_i%5Csum_%7Bk%7D%5EMp_k%5Cright)%5Cfrac%7B%5Cpartial%5Csum_j%5EN%20w_%7Bij%7Dx_j%7D%7B%5Cpartial%20w_%7Bij%7D%7D%5C%5C%20%26%3D%5Cleft(p_i-q_i%5Csum_%7Bk%7D%5EMp_k%5Cright)x_j%20%5Cend%7Bsplit%7D"><br>\n    <img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_k">是一个结构为[0,0,0,1,0,0......]的只有一个元素是1其余元素为0的数组，所以它的合计为1，因此得：\n</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D%20%3D%20%5Cleft(p_i-q_i%5Cright)x_j"></p>\n<p>虽然推导这个求偏导的过程要花费一些功夫，但是这个结果却非常简单——<strong>真实分布与预测分布的差值乘权重参数对应的特征值</strong>。如果交叉熵函数中使用了负号，那么导函数为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D%3D(q_i-p_i)x_j">，很多文章更喜欢用这种求最小值的方式。<br>\n    观察<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D">的表达式，<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=p_i">和<img alt="机器学习——softmax计算"\n                                                                                        src="https://math.jianshu.com/math?formula=x_j">都是已知的数值，在优化的过程中只有<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=q_i">会发生改变。所以当预测分布越接近真实分布时增量会越来越接近0。</p>\n\n<h4 id="h4-9">2.多个样本与矩阵运算</h4>\n<p>上面求导的过程并没有考虑多个样本的情况，设现在有O个样本。那么求导公式变成：<br> <img alt="机器学习——softmax计算"\n                                                   src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D%3D%5Cfrac%7B1%7D%7BO%7D%5Csum_k%5EO(p_%7B(k)i%7D-q_i)x_%7B(k)j%7D">。<br>\n    因为每一个子项的求导结果都是向0接近，所以求和再平分之后也是靠近0的。<br> 现在模型参数的更新公式用矩阵表示为：<br> <img alt="机器学习——softmax计算"\n                                                                        src="https://math.jianshu.com/math?formula=W%3DW%2B%5Ceta%20D">。其中<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=D">是<img alt="机器学习——softmax计算"\n                                                                                      src="https://math.jianshu.com/math?formula=%5Cnabla_%7Bij%7D">的矩阵形，<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=%5Ceta">是一个常量，<img alt="机器学习——softmax计算"\n                                                                                                src="https://math.jianshu.com/math?formula=W">是<img\n            alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=w_%7Bij%7D">的矩阵形。<br>\n    设P表示样本真实分布的矩阵（即标记矩阵），Q是文章前面介绍的softmax矩阵计算的结果，X表示样本矩阵。那么D的矩阵表示为：<img alt="机器学习——softmax计算"\n                                                                        src="https://math.jianshu.com/math?formula=D%3D(P-Q)%5ETX">。\n</p>\n\n<h2 id="h2-7">计算法则总结与编码实现</h2>\n\n<h3 id="h3-6">算法总结</h3>\n<p>经过前面推导分析，softmax机器学习算法建模分为以下几项内容。</p>\n\n<h4 id="h4-10">1.定义</h4>\n<p>有<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=O">个样本、<img alt="机器学习——softmax计算"\n                                                                                      src="https://math.jianshu.com/math?formula=N">个特征、<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=M">个分类，。<br> <img alt="机器学习——softmax计算"\n                                                                                           src="https://math.jianshu.com/math?formula=X">是样本（feature）矩阵，形状为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=(O%2CN)"><br> <img alt="机器学习——softmax计算"\n                                                                                            src="https://math.jianshu.com/math?formula=W">是权重矩阵，形状为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=(M%2CN)"><br> <img alt="机器学习——softmax计算"\n                                                                                            src="https://math.jianshu.com/math?formula=P">是标签（label）矩阵，形状为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=(O%2CM)"><br> <img alt="机器学习——softmax计算"\n                                                                                            src="https://math.jianshu.com/math?formula=Q">是softmax计算后得到的矩阵，形状为<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=(O%2CM)"><br> <img alt="机器学习——softmax计算"\n                                                                                            src="https://math.jianshu.com/math?formula=E_1%2CE_2">是两个用于合并计算的单位矩阵，形状为(M,1)和(O,1)，<img\n        alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=E%3D%5Cbegin%7Bbmatrix%7D%201%5C%5C1%5C%5C%20%5Ccdots%20%5C%5C1%20%5Cend%7Bbmatrix%7D"><br>\n    矩阵<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=A%5ET">表示转置矩阵，<img alt="机器学习——softmax计算"\n                                                                                               src="https://math.jianshu.com/math?formula=A%5ED">表示取矩阵的对角线元素（类似于特征）。\n</p>\n\n<h4 id="h4-11">2.softmax计算</h4>\n<p>权重指数：<img alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=G%3De%5E%7BXW%5ET%7D"><br> 归一化：<img\n        alt="机器学习——softmax计算" src="https://math.jianshu.com/math?formula=Q%3DSoftmax%3D%5Cfrac%7BG%7D%7BG*E_1%7D"></p>\n\n<h4 id="h4-12">3.损失函数</h4>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=Loss%3D%5Cfrac%7B1%7D%7Bo%7D%5Cleft%5BPlog(Q%5ET)%5Cright%5D%5ED%C3%97E">\n</p>\n\n<h4 id="h4-13">4.参数训练</h4>\n<p><img alt="机器学习——softmax计算"\n        src="https://math.jianshu.com/math?formula=W%3DW%2B%5Ceta%5Cfrac%7B%5Cleft(P-Q%5Cright)%5ET%C3%97X%7D%7BO%7D">，训练会重复这个计算，直到变化率“接近”0。\n</p>\n\n<h3 id="h3-7">编码实现</h3>\n<p>以下代码在<a href="https://github.com/chkui/ml-math-softmax" target="_blank" rel="nofollow">https://github.com/chkui/ml-math-softmax</a>。<br>\n    如下图，<code>sample.softmax_train.softmax_modual.Softmax</code>类模拟了一个softmax机器学习的过程。</p>\n<pre><code class="ruby">import numpy as np\n\n<span class="hljs-class"><span class="code-keyword">class</span> <span class="code-title">Softmax</span>:</span>\n    <span class="hljs-function"><span class="code-keyword">def</span> <span class="code-title">__init__</span><span\n            class="hljs-params">(<span class="code-keyword">self</span>, features, labels)</span></span>:\n        <span class="code-keyword">self</span>.__features = features\n        <span class="code-keyword">self</span>.__labels = labels\n        <span class="code-keyword">self</span>.__weight = np.zeros((labels.shape[<span class="hljs-number">1</span>], features.shape[<span\n            class="hljs-number">1</span>]))\n        <span class="code-comment"># 用于 softmax 归一化计算分布的标量矩阵</span>\n        <span class="code-keyword">self</span>.__e_softmax = np.ones((labels.shape[<span class="hljs-number">1</span>], <span\n            class="hljs-number">1</span>))\n        <span class="code-comment"># 用于 损失函数计算的标量矩阵</span>\n        <span class="code-keyword">self</span>.__e_loss = np.ones((features.shape[<span\n            class="hljs-number">0</span>], <span class="hljs-number">1</span>))\n        <span class="code-comment"># flag用于标记运算符号</span>\n        <span class="code-comment"># flag如果是-1,那么损失函数就是求最小值，那么优化器求差值。</span>\n        <span class="code-comment"># flag如果是+1损失函数就是求最大值，那么优化器求和</span>\n        <span class="code-keyword">self</span>.__flag = <span class="hljs-number">1</span>\n\n    <span class="hljs-function"><span class="code-keyword">def</span> <span class="code-title">__softmax</span><span\n            class="hljs-params">(<span class="code-keyword">self</span>)</span></span>:\n        liner = <span class="code-keyword">self</span>.__features * <span class="code-keyword">self</span>.__weight.T\n        exp = np.exp(liner)\n        den = exp * <span class="code-keyword">self</span>.__e_softmax\n        q = exp / den\n        <span class="code-keyword">return</span> q\n\n    <span class="hljs-function"><span class="code-keyword">def</span> <span class="code-title">__loss</span><span\n            class="hljs-params">(<span class="code-keyword">self</span>, q)</span></span>:\n        h = <span class="code-keyword">self</span>.__labels * np.log(q.T)\n        h = h.diagonal()\n        loss = <span class="code-keyword">self</span>.__flag * h * <span\n            class="code-keyword">self</span>.__e_loss / <span class="code-keyword">self</span>.__e_loss.shape[<span\n            class="hljs-number">0</span>]\n        <span class="code-keyword">return</span> loss\n\n    <span class="hljs-function"><span class="code-keyword">def</span> <span class="code-title">__optimizer</span><span\n            class="hljs-params">(<span class="code-keyword">self</span>, q, step)</span></span>:\n        d = ((<span class="code-keyword">self</span>.__flag * <span class="code-keyword">self</span>.__labels - <span\n            class="code-keyword">self</span>.__flag * q).getT() * <span\n            class="code-keyword">self</span>.__features) / <span class="code-keyword">self</span>.__features.shape[<span\n            class="hljs-number">0</span>]\n        <span class="code-keyword">self</span>.__weight = <span class="code-keyword">self</span>.__weight + (<span\n            class="code-keyword">self</span>.__flag * step) * d\n\n    <span class="hljs-function"><span class="code-keyword">def</span> <span class="code-title">train</span><span\n            class="hljs-params">(<span class="code-keyword">self</span>, handle, repeat=<span\n            class="hljs-number">2000</span>, step=<span class="hljs-number">0</span>.<span class="hljs-number">1</span>)</span></span>:\n        <span class="code-string">""</span><span class="code-string">"\n        训练\n        :param handle: 单轮训练的回调，用于输出各项数据 (count, loss, )\n        :param repeat: 重复的轮次,每轮会执行一次存储 2000\n        :param step: 优化器步近量\n        :return:\n        "</span><span class="code-string">""</span>\n        print(<span class="code-string">"Weight shape={}"</span>.format(<span class="code-keyword">self</span>.__weight.shape))\n        count = <span class="hljs-number">0</span>\n        <span class="code-keyword">while</span> count &lt; <span class="hljs-symbol">repeat:</span>\n            q = <span class="code-keyword">self</span>.__softmax()\n            loss = <span class="code-keyword">self</span>.__loss(q)\n            <span class="code-keyword">self</span>.__optimizer(q, step)\n            count = count + <span class="hljs-number">1</span>\n            handle(count, loss)\n</code></pre>\n<p>\n    类中的<code>__softmax</code>、<code>__loss</code>、<code>__optimizer</code>方法分别对应前面介绍的三步计算（归一化，损失函数，参数优化），而在<code>train</code>方法中就是重复调用这三个方法来不断的优化权重参数。<br>\n    为了执行训练<a href="https://github.com/chkui/ml-math-softmax/blob/master/sample/softmax_train/random_data.py"\n             target="_blank"\n             rel="nofollow"><code>sample.softmax_train.random_data.RandomData</code></a>用于随机生成<em>样本特征</em>和<em>样本标签</em>数据。<br>\n    下图展示了执行5000次优化过程中<strong>Loss</strong>的变化趋势：</p>\n<p><img alt="机器学习——softmax计算"\n        src="https://upload-images.jianshu.io/upload_images/2418406-b3452c5dfefc46a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"\n        class="zoom-in-cursor"></p>\n<p>训练次数与损失函数的输出</p>\n<p><br> <strong>Count</strong>表示执行训练的次数，<strong>Loss</strong>表示损失函数的输出值，可以发现几个特点：</p>\n<p></p>\n<ol>\n    <li>在优化的过程中<em>Loss</em>是逐渐接近0的。</li>\n    <li>反复使用相同的样本（案例中随机生成了500个样本）优化器在前1000次有比较明显的效果，但是后续增长乏力。</li>\n</ol>\n<p>由于使用的是随机数据，所以收敛的效果并不太理想，但是总的趋势还是收敛。后续的博文中本人会使用MNIST之类的真实数据来测试验证softmax。</p>\n<p>Github的代码中除了<a href="https://github.com/chkui/ml-math-softmax/tree/master/sample/softmax_train" target="_blank"\n                  rel="nofollow"><code>softmax_train</code></a>用于演示训练和收敛的效果，还有<a\n        href="https://github.com/chkui/ml-math-softmax/tree/master/sample/softmax_estimator" target="_blank"\n        rel="nofollow"><code>softmax_estimator</code></a>和<a\n        href="https://github.com/chkui/ml-math-softmax/blob/master/sample/softmax_compute.py" target="_blank"\n        rel="nofollow"><code>softmax_compute</code></a>。前者提供了参数相关的磁盘操作，后者简单展示了softmax算法的编码实现，需要了解的可以到代码库中查看。</p>'}});