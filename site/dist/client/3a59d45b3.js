webpackJsonp([3],{329:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='<p>在hazelcast的官方文档中，提到了其支持read-through，write-through与write-behind三种模式。查阅资料，最后在oracle的官文中找到了比较靠谱的解释。</p>\n<p>read-throug、write-through、write-behind三个概念都是关于数据缓存管理的。其实这些概念在实际使用的过程中经常接触。</p>\n<h2 id="h2-1"><strong>Read-throug</strong></h2>\n<p>当应用系统向缓存系统请求数据时（例如使用key=x向缓存请求数据）；如果缓存中并没有对应的数据存在（key=x的value不存在），缓存系统将向底层数据源的读取数据。如果数据在缓存中存在（命中key=x），则直接返回缓存中存在的数据。这就是所谓的<strong>Read-throug。</strong></p>\n<p>hazelcast原文：</p>\n<blockquote>\n    <p>If an entry does not exist in the memory when an application asks for it, Hazelcast asks your loader implementation to load that entry from the data store。 &nbsp;If the entry exists there, the loader implementation gets it, hands it to Hazelcast, and Hazelcast puts it into the memory. This is read-through persistence mode。</p>\n</blockquote>\n<p>下图是Oracle官网的<strong>Read-throug</strong>图例。\n    <img alt="Hazelcast read-through、write-through与write-behind模式" src="https://file.mahoooo.com/res/file/read_through_write_through_and_write_behind_1.jpg">\n</p>\n<h2 id="h2-2">Write-Through</h2>\n<p>当应用系统对缓存中的数据进行更新时（例如调用put方法更新或添加条目），缓存系统会同步更新缓存数据和底层数据源。</p>\n<p>下图展示了执行过程：</p>\n<p><img alt="Hazelcast read-through、write-through与write-behind模式" src="https://file.mahoooo.com/res/file/read_through_write_through_and_write_behind_2.jpg"></p>\n<h2 id="h2-3">Write-Behind</h2>\n<p>当应用系统对缓存中的数据进行更新时（例如调用put方法更新或添加条目），缓存系统会在指定的时间后向底层数据源更新数据。</p>\n<p><img alt="Hazelcast read-through、write-through与write-behind模式" src="https://file.mahoooo.com/res/file/read_through_write_through_and_write_behind_3.jpg"></p>'},342:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='<h2 id="h2-1">jolokia架构</h2>\n<p>虽然jolokia是为了满足JSR-160的要求，但是他和JSR-160连接器有巨大的差异。其中最引人注目的区别是jolokia传递数据是无类型的数据（<span style="color:#FF8C00"><em>说白了就是使用了Json数据传递，替代了RMI传递Java序列化数据的方式</em></span>）。\n</p>\n<p>2003年提交的JSR-160规定客户端可以透明的调用MBean服务，无论被调用的MBean是驻留在本地还是在远程的MBean服务中。这样做的好处是提供了一个简洁通用的Java\n    API接口。但是JSR-160的实现存在许多问题：</p>\n<ol>\n    <li>它非常危险，因为它隐性暴露了JMX的远程接口。</li>\n    <li>它还存在性能问题。无论是远程还是本地调用，调用者至少要知道调用过程是怎么样的、会收到什么结果。在实际使用时，需要有明确的远程消息传递模式，让调用者知道现在是在使用响应较慢的远程调用。</li>\n    <li>使用RMI（<em><span style="color:#FF8C00">JSR-160连接器的默认协议栈</span></em>）时需要使用Java对象的序列化与反序列化机制来构建传递管道。这样做就阻碍了Java技术栈之外的环境来使用它。\n    </li>\n</ol>\n<p>以上3个原因大概就是RMI（<span style="color:#FF8C00"><em>JSR-160连接器的默认协议栈</em></span>）在远程传输协议上逐渐失去市场份额的原因。</p>\n<p>Jolokia是无类型的数据，使用了Json这种轻量化的序列化方案来替代RMI方案。使用这样的方法当然存在一些缺点（<em>比如需要额外增加一层代理</em>），但是带来了一些优势，至少这样的实现方案在JMX世界是独一无二的。</p>\n\n<h2 id="h2-2">Jolokia植入模式（Agent mode）</h2>\n<p><img alt="Jolokia——架构与使用介绍" height="489"\n        src="https://file.mahoooo.com/res/file/jolokia_how_to_use_2.png" width="666"></p>\n<p>上如展示了Jolokia 植入模式的体系结构，说明了与之有关的运行环境。</p>\n<p>Jolokia植入模式是在本地基于http协议提供了一个使用Json作为数据格式的外部接口，此时Jolokia会桥接到本地的JMX\n    MBeans接口。Jolokia使用http服务扩展了JSR-160，因此需要针对Jolokia的运行进行一些额外的处理。多种技术可以工作于http协议，最常规的方法是将jolokia放置到servlet容器中，比如Tomcat或Jetty，这样Jolokia完全可以看做是一个常规的Java\n    web应用，让所有的开发人员都能够很好理解并快速的从中读取数据。</p>\n<p>当然还有更多的方式使用Jolokia植入，比如使用OSGi HttpService或嵌入到有Jetty-Server的应用中。Jvm代理者需要使用Java1.6以上版本，在他运行时，可以连接到任何本地运行的Java进程。</p>\n<p><span style="color:#FF8C00"><em>附注——关于“植入模式”的称呼的说明：官方名为“Agent mode”，按照字面意思应该译为“代理者模式”。但是后面又一个模式叫代理模式（Proxy Mode），为了更便于理解和表达中文意思，这里命名其为“植入模式”。</em></span>\n</p>\n\n<h2 id="h2-3">Jolokia代理模式</h2>\n<p>代理模式用于无法将Jolokia部署到目标平台上（说白了就是无法部署到同一台服务器）。在这个模式下，唯一可用的方式就是目标服务开启了JSR-160连接。这样做大部分是规范原因（原文是“political\n    reasons”——政治原因-_-）——有时候根本不允许在目标服务器部署一个额外的软件系统，或者是这样做需要等待一个漫长的审批流程。还有一个原因是目标服务器已经通过RMI开启了JSR-160连接，并且我们不想额外再去在本地部署Jolokia。</p>\n<p>可以将jolokia.war部署到servlet容器中（这个war包也可用于植入模式）。下图是一个典型的代理模式架构。</p>\n<p><img alt="Jolokia——架构与使用介绍" height="439"\n        src="https://file.mahoooo.com/res/file/jolokia_how_to_use_1.png" width="617"></p>\n<p>一个jolokia客户端发送常规的请求到jolokia代理服务，这个请求包含了额外的数据用于标记要查询的目标。所有的路由信息包含在请求信息中，使得代理服务无需特别的配置即可工作。</p>\n\n<h2 id="h2-4">结尾</h2>\n<p>如果没有什么特别的限制，优先使用植入模式。植入模式比代理模式有更多的优势，因为他没有附加层、减少了维度成本和技术复杂性、而且性能也优于代理模式。此外，一些jolokia特性也无法在代理模式中使用，例如“merging of\n    MBeanServers”。</p>'},343:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='\n<p>通常情况下，图形界面的发行版 <em><strong>linux</strong></em> 可以在 <strong><em>Setting-&gt;Device-&gt;Display</em></strong> 中直接设置多个屏幕的分辨率。但是坑总是无处不在的，有时候明明用得好好的分辨率就出毛病了，而且不能在界面上设置。此时可以通过 <em><strong>xrandr</strong></em> 命令来直接设置分辨率。</p>\n\n<h2 id="h2-1">常规方法</h2>\n<p>1.查看显示模式参数：</p>\n<pre class="bash"><code class="language-bash"><span class="code-comment"><span class="code-comment">#1440 900就是要修改的分辨率，根据需要可以使用1920 1080、1720 900等</span></span>\n<span class="code-variable"><span class="code-variable">$cvt</span></span> 1440 900\n<span class="code-comment"><span class="code-comment">#输出内容</span></span>\n1440x900 59.89 Hz (CVT 1.30MA) hsync: 55.93 kHz; pclk: 106.50 MHz\nModeline <span class="code-string"><span class="code-string">"1440x900_60.00"</span></span>  106.50  1440 1528 1672 1904  900 903 909 934 -hsync +vsync</code></pre>\n<p>Modeline之后的内容就是我们后面添加显示模式要使用的参数。</p>\n<p>2.通过addMode命令增加一个现实模式：</p>\n<pre class="bash"><code class="language-bash"><span class="code-comment"><span class="code-comment">#将Modeline的阿才能书复制到 --newmode之后即可</span></span>\n<span class="code-comment"><span class="code-comment">#后面的参数可以根据需要调整，请查阅cvt相关的说明</span></span>\n<span class="code-variable"><span class="code-variable">$xrandr</span></span> --newmode <span class="code-string"><span class="code-string">"1440x900_60.00"</span></span>  106.50  1440 1528 1672 1904  900 903 909 934 -hsync +vsync</code></pre>\n<p>3.增加到对应的显示器。</p>\n<p>先用xrandr命令查询对应的显示器：</p>\n<pre class="scss"><code class="language-bash"><span class="code-variable"><span class="code-variable">$xrandr</span></span>\n<span class="code-comment">#输出类似以下的内容</span>\nWAYLAND0 connected primary 1366x768+0+0 (<span class="code-attribute">normal</span> <span class="code-attribute">left</span> inverted <span class="code-attribute">right</span> x axis y axis) 309mm x 173mm\n   1366x768      60<span class="code-selector-class">.00</span>*+\n   1360x768      59<span class="code-selector-class">.80</span>    59<span class="code-selector-class">.96</span>\n   1024x768      60<span class="code-selector-class">.04</span>    60<span class="code-selector-class">.00</span>\n   960x720       60<span class="code-selector-class">.00</span>\n   928x696       60<span class="code-selector-class">.05</span>\nWAYLAND1 connected 1024x768+1366+0 (<span class="code-attribute">normal</span> <span class="code-attribute">left</span> inverted <span class="code-attribute">right</span> x axis y axis) 0mm x 0mm\n   1024x768      60<span class="code-selector-class">.00</span>*\n   800x600       60<span class="code-selector-class">.32</span>    56<span class="code-selector-class">.25</span>\n   848x480       60<span class="code-selector-class">.00</span>\n   640x480       59<span class="code-selector-class">.94</span>\n</code></pre>\n<p>记住WAYLAND0\\WAYLAND1的名称，这是我们显示器的代理名称。通常用手提的话WAYLAND0就是对应的手提电脑的显示器、其余的都是扩增屏幕。</p>\n<p>然后向对应显示器下增加一个模式：</p>\n<pre class="bash"><code class="language-bash"><span class="code-variable"><span class="code-variable">$xrandr</span></span> --addmode WAYLAND1 <span class="code-string"><span class="code-string">"1440x900_60.00"</span></span></code></pre>\n<p>然后就OK了..........</p>\n\n<h2 id="h2-2">遇到的问题</h2>\n<p>但是，要是天底下的事情都这么轻松世界就完美了。</p>\n\n<h3 id="h3-1">问题一，xrand命令指针对当前用户</h3>\n<p>在使用 xrand命令时切记是针对当前用户的。例如我就是创建了一个管理员用户，然后把root账户禁用了。在执行命令时习惯性的加 <strong><em>sudo</em></strong>，最后会输出类似于 <em>"MIT-MAGIC-COOKIE-1 keyCan\'t open display :0.0" </em>这样的内容。</p>\n\n<h3 id="h3-2"><strong><em>问题二，最后一步输出 xrandr: Configure crtc 0 failed</em></strong></h3>\n<p>在最后一行&nbsp;<strong><em>$xrandr --addmode WAYLAND1 "1440x900_60.00" </em></strong>命令之后并没有修改成功，而是输出了<strong><em>xrandr: Configure crtc 0 failed&nbsp;</em></strong>或&nbsp;<strong><em>xrandr: Configure crtc 1 failed</em></strong>这样的内容。查阅了一圈资料。在askubuntu找到说明。据说是升级到Ubuntu17.10之后，使用AMD/ATI的显卡容易遇到这个坑，wayland识别不了显示器。这个时候打开Setting的Displays面板显示的是 Unknown Display，最高分辨率只能到1024*768，最不费劲的方法是可以通过安装新的驱动源来解决（但是我安装了一次还是不行）。</p>\n<p>实际上，只要显卡和显示器都支持某个分辨率，直接告诉显卡按照这个分辨率输出图像就好了，不用wayland去识别显示器。</p>\n<p>首先关闭wayland服务，wayland是17.10之后新用的显示器服务（据说是可以在登陆界面选择和X.org切换，但是我的居然没有）。关闭方法：</p>\n<p>1.打开配置文件：</p>\n<pre class="nginx"><code class="language-bash"><span class="code-comment"><span class="code-comment">#打开custom.conf文件，不同发行版文件位置可能有差异</span></span>\n<span class="code-attribute">vim</span> /etc/gdm3/custom.conf</code></pre>\n<p>2.修改文件，找到 <strong><em>#WaylandEnable=false</em></strong> 这一行，然后去掉注释。</p>\n<pre class="ini"><code class="language-bash"><span class="code-comment"><span class="code-comment"># Uncoment the line below to force the login screen to use Xorg</span></span>\n<span class="hljs-attr">WaylandEnable</span>=<span class="hljs-literal"><span class="hljs-literal">false</span></span>\n<span class="code-comment"><span class="code-comment">#......</span></span></code></pre>\n<p>3.最后reboot重启电脑。</p>\n<p>4.启动完成后输入 xrandr 命令发现之前的&nbsp;WAYLAND0 和&nbsp;WAYLAND1 变成了输出端口的名称：</p>\n<pre class="swift"><code class="language-bash"><span class="code-variable">$xrandr</span>\n<span class="code-comment">#输出</span>\n<span class="code-type">Screen</span> <span class="hljs-number">0</span>: minimum <span class="hljs-number">320</span> x <span class="hljs-number">200</span>, current <span class="hljs-number">2806</span> x <span class="hljs-number">900</span>, maximum <span class="hljs-number">8192</span> x <span class="hljs-number">8192</span>\neDP-<span class="hljs-number">1</span> connected primary 1366x768+<span class="hljs-number">0</span>+<span class="hljs-number">0</span> (normal <span class="code-keyword">left</span> inverted <span class="code-keyword">right</span> x axis y axis) 309mm x 173mm\n   1366x768      <span class="hljs-number">60.00</span>*+\n   1360x768      <span class="hljs-number">59.80</span>    <span class="hljs-number">59.96</span>\n   1024x768      <span class="hljs-number">60.04</span>    <span class="hljs-number">60.00</span>\n   960x720       <span class="hljs-number">60.00</span>\n   928x696       <span class="hljs-number">60.05</span>\n<span class="code-type">HDMI</span>-<span class="hljs-number">1</span> disconnected (normal <span class="code-keyword">left</span> inverted <span class="code-keyword">right</span> x axis y axis)\n<span class="code-type">DP</span>-<span class="hljs-number">1</span> connected 1440x900+<span class="hljs-number">1366</span>+<span class="hljs-number">0</span> (normal <span class="code-keyword">left</span> inverted <span class="code-keyword">right</span> x axis y axis) 0mm x 0mm\n   1024x768      <span class="hljs-number">60.00</span>\n   800x600       <span class="hljs-number">60.32</span>    <span class="hljs-number">56.25</span>\n   848x480       <span class="hljs-number">60.00</span>\n   640x480       <span class="hljs-number">59.94</span>\n<span class="code-type">HDMI</span>-<span class="hljs-number">2</span> disconnected (normal <span class="code-keyword">left</span> inverted <span class="code-keyword">right</span> x axis y axis)</code></pre>\n<p>这里的DP-1就是我用的外接显示器，但是现在只能显示到1024*768。然后按照前面介绍的方法依次执行<em><strong>$cvt 1440 900</strong></em>、<strong><em>$xrandr newmode</em></strong>、<em><strong>$xrandr addmode DP-1 "1440x900_60.00"</strong></em>，仅仅是最后addmode的参数有些许差异。</p>\n<p>修改后，在Displays里可以看到最新的分辨率。</p>\n\n<h2 id="h2-3">永久性问题</h2>\n<p>最后，用这个方法有个最大的问题是没法保存。每次重启过后还是没法识别显示器的分辨率，又回到修改之前的状态。</p>\n<p>有些地方说可以像下面这样添加或修改&nbsp;/etc/X11/xorg.conf&nbsp; 文件：</p>\n<pre class="nginx"><code class="nginx"><span class="code-attribute"><span class="code-attribute">Section</span></span> <span class="code-string"><span class="code-string">"Monitor"</span></span>\nIdentifier <span class="code-string"><span class="code-string">"Configured Monitor"</span></span>\nModeline <span class="code-string"><span class="code-string">"1920x1080_60.00"</span></span>  <span class="hljs-number"><span class="hljs-number">173</span></span>.<span class="hljs-number"><span class="hljs-number">00</span></span>  <span class="hljs-number"><span class="hljs-number">1920</span></span> <span class="hljs-number"><span class="hljs-number">2048</span></span> <span class="hljs-number"><span class="hljs-number">2248</span></span> <span class="hljs-number"><span class="hljs-number">2576</span></span>  <span class="hljs-number"><span class="hljs-number">1080</span></span> <span class="hljs-number"><span class="hljs-number">1083</span></span> <span class="hljs-number"><span class="hljs-number">1088</span></span> <span class="hljs-number"><span class="hljs-number">1120</span></span> -hsync +vsync\nOption <span class="code-string"><span class="code-string">"PreferredMode"</span></span> <span class="code-string"><span class="code-string">"1920x1080_60.00"</span></span>\nEndSection\nSection <span class="code-string"><span class="code-string">"Screen"</span></span>\nIdentifier <span class="code-string"><span class="code-string">"Default Screen"</span></span>\nMonitor <span class="code-string"><span class="code-string">"Configured Monitor"</span></span>\nDevice <span class="code-string"><span class="code-string">"Configured Video Device"</span></span>\nEndSection\nSection <span class="code-string"><span class="code-string">"Device"</span></span>\nIdentifier <span class="code-string"><span class="code-string">"Configured Video Device"</span></span>\nEndSection</code></pre>\n<p>但是我修改之后没有任何效果。而且还多出一份&nbsp;/etc/X11/xorg.conf.failsafe 文件。不知道是不是17.0.4特有的原因，毕竟我也是将系统升级为17.0.4之后分辨率才出问题的。</p>\n<p>既然 xorg.conf 配置无法生效，我直接将命令创建为一个shell，然后开机运行即可：</p>\n<pre class="css"><code class="language-bash"><span class="code-comment"><span class="code-selector-id">#view-port-init</span><span class="code-selector-class">.sh</span></span>\n<span class="code-selector-tag">xrandr</span> <span class="code-selector-tag">--newmode</span> <span class="code-string">"1920<span class="code-selector-tag">x1080_60</span><span class="code-selector-class">.00</span>"</span>  173<span class="code-selector-class">.00</span>  1920 2048 2248 2576  1080 1083 1088 1120 <span class="code-selector-tag">-hsync</span> +<span class="code-selector-tag">vsync</span>\n<span class="code-selector-tag">xrandr</span> <span class="code-selector-tag">--addmode</span> <span class="code-selector-tag">DP-1</span> <span class="code-string">"1920<span class="code-selector-tag">x1080_60</span><span class="code-selector-class">.00</span>"</span>\n<span class="code-comment"><span class="code-selector-id">#DP-1</span>这里请根据自己的参数修改。</span>\n</code></pre>\n<p>参考：<a href="https://askubuntu.com/questions/136139/xrandr-configure-crtc-0-failed-when-trying-to-change-resolution-on-external-m?answertab=active#tab-top" rel="nofollow">xrandr-configure-crtc-0-failed-when-trying-to-change-resolution</a>。</p>\n'},346:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='<p>使用react到现在，让人头疼的一个问题是安装node-sass。其实导致问题的根源在于安装过程需要下载一个binding.node文件，而因“你懂的”原因，访问不了这个地址。根据这个原因，总结了以下几个解决方案：</p>\n<h2 id="h2-1">翻墙</h2>\n<p>这没什么好说的了，这是最轻松最彪悍的解决方案。只要能翻墙直接一个&nbsp;<span style="background-color:#D3D3D3">&nbsp;npm i&nbsp;</span>命令就完事了，什么都不必去操心。至于怎么翻墙…………\n</p>\n\n<h2 id="h2-2">使用cnpm</h2>\n<p>cnpm是一个强悍的工具，几乎能解决所有npm安装第三方包时遇到的问题。执行如下命令即可：</p>\n<pre class="sql"><code class="language-bash">npm <span class="code-keyword">install</span>\nnpm rm node-sass\ncnpm <span class="code-keyword">install</span> node-sass\nnpm <span class="code-keyword">install</span></code></pre>\n<p>\n    但是我们在使用cnpm时也遇到了一个坑，在ubuntu14.04打出来的包运行报错，不用cnpm下载居然就不会有这问题，由于没有时间，没有详细去了解原因是什么。如果你是Windows开发而使用Linux环境打包或运行，可能会碰到这个问题。</p>\n\n<h2 id="h2-3">下载后编译</h2>\n<p>\n    实际上为了得到binding.node，是可以直接从github上把源码下载下来之后再编译出来的，node-sass自己也会这样做，但是编译要依赖其他工具。在各种发行版的linux下还好，几乎所有需要的环境（python等）都是预安装的，如果是root权限直接<span\n        style="background-color:#A9A9A9"> upm install </span>就搞定了，所以有时候根本感觉不到这个问题。在windows下就得花时间根据install时的错误日志了解还要安装什么。\n</p>\n<p>linux下遇到权限问题请执行：</p>\n<pre class="lua"><code class="language-bash">npm i <span class="code-comment">--unsafe -perm</span></code></pre>\n\n<h2 id="h2-4">附送一个最奇葩的坑</h2>\n<p>这是我用所有的开源工具遇到过最奇葩的坑。我将一个文件命名为“./dropDown.scss”，然后在linux(是ubuntu\n    14.04其他发行版没时间去测试，windows没这毛病)上用webpack打包，打包过程没有任何异常，但是放到服务器上运行打开某个页面就抛出无法找到"./dropDown.scss"的异常，然后node直接停机了········。我前后跟进了2天寻找问题的原因。最后突发奇想将文件名由dropDown.scss修改为pullDown.scss后一切都好了。我强烈的怀疑是某个临时工在node-sass里写了什么“硬编码”对字符串进行判断，发现“/drop[*]”这样的前缀进行一些特殊处理。在此记录下来。</p>'},367:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='<h3 id="h3-1">非受控组件（Uncontrolled Components）</h3>\n<h4 id="h4-1">使用非受控组件</h4>\n<p>在大部分情况下，推荐使用 <a title="受控组件" href="https://www.chkui.com/article/react/react_list_key_and_form#h1-2">受控组件</a> 来实现表单、输入框等状态控制。在受控组件中，表单等数据都有React组件自己处理。这里将介绍另外一种非受控组件，表单的数据有Dom自己控制。</p>\n<p>非受控组件实现的重点是用Refs特性获取真实Dom来代替每次数据变更去更新组件的状态值。</p>\n<p>例如下面的代码，在非受控组件中记录被用户输入的名字：</p>\n<pre class="scala"><code class="language-javascript"><span class="hljs-class"><span class="code-keyword"><span class="hljs-class"><span class="code-keyword">class</span></span></span><span class="hljs-class"> </span><span class="code-title"><span class="hljs-class"><span class="code-title">NameForm</span></span></span><span class="hljs-class"> </span><span class="code-keyword"><span class="hljs-class"><span class="code-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="code-title"><span class="hljs-class"><span class="code-title">React</span></span></span><span class="hljs-class">.</span><span class="code-title"><span class="hljs-class"><span class="code-title">Component</span></span></span><span class="hljs-class"> </span></span>{\n  <span class="code-keyword">constructor</span>(props) {\n    <span class="code-keyword"><span class="code-keyword">super</span></span>(props);\n    <span class="code-keyword"><span class="code-keyword">this</span></span>.handleSubmit = <span class="code-keyword"><span class="code-keyword">this</span></span>.handleSubmit.bind(<span class="code-keyword"><span class="code-keyword">this</span></span>);\n  }\n\n  handleSubmit(event) {\n    <span class="code-comment"><span class="code-comment">//在提交时，直接使用ref获取的真实Dom获取值</span></span>\n    alert(<span class="code-string"><span class="hljs-symbol">\'A</span> name was submitted: \'</span> + <span class="code-keyword"><span class="code-keyword">this</span></span>.input.value);\n    event.preventDefault();\n  }\n\n  render() {\n    <span class="code-keyword"><span class="code-keyword">return</span></span> (\n      &lt;form onSubmit={<span class="code-keyword">this</span>.handleSubmit}&gt;\n        &lt;label&gt;\n          <span class="code-type">Name</span>:\n          &lt;input <span class="hljs-class"><span class="code-keyword">type</span></span>=<span class="code-string">"text"</span> ref={(input) =&gt; <span class="code-keyword">this</span>.input = input} /&gt;\n        &lt;/label&gt;\n        &lt;input <span class="hljs-class"><span class="code-keyword">type</span></span>=<span class="code-string">"submit"</span> value=<span class="code-string">"Submit"</span> /&gt;\n      &lt;/form&gt;\n    );\n  }\n}</code></pre>\n<p><a title="代码测试" href="https://codepen.io/gaearon/pen/WooRWa?editors=0010" rel="nofollow">尝试代码</a>。</p>\n<p>由于在非受控组件中使用Refs特性获取了真实Dom的实例，所以在使用非受控组建时，更容易集成React和非React代码，在某些时候也可以省略一些代码。但是建议除了特殊情况，都使用受控组件。</p>\n<p>如果想要深入理解什么情况下使用哪种组件，建议阅读 <a title="受控组件与非受控组件的差异" href="https://goshakkk.name/controlled-vs-uncontrolled-inputs-react/" rel="nofollow">受控和不受控表单输入</a> 一文。</p>\n<h4 id="h4-2">组件默认值</h4>\n<p>在React渲染的生命周期，表单中的value属性会被覆盖Dom中的value值。在使用非受控组件时，通常需要React设定一个默认初始值但是不再控制后续更新。基于这个案例，你可以指定一个<code>defaultValue</code>&nbsp;属性来代替&nbsp;<code>value</code>。</p>\n<pre class="xml"><code class="language-javascript">render() {\n  <span class="code-keyword">return</span> (\n    <span class="code-tag">&lt;<span class="code-name">form</span> <span class="hljs-attr">onSubmit</span>=<span class="code-string">{this.handleSubmit}</span>&gt;</span>\n      <span class="code-tag">&lt;<span class="code-name">label</span>&gt;</span>\n        Name:\n        <span class="code-tag">&lt;<span class="code-name">input</span>\n          <span class="hljs-attr">defaultValue</span>=<span class="code-string">"Bob"</span>\n          <span class="hljs-attr">type</span>=<span class="code-string">"text"</span>\n          <span class="hljs-attr">ref</span>=<span class="code-string">{(input)</span> =&gt;</span> this.input = input} /&gt;\n      <span class="code-tag">&lt;/<span class="code-name">label</span>&gt;</span>\n      <span class="code-tag">&lt;<span class="code-name">input</span> <span class="hljs-attr">type</span>=<span class="code-string">"submit"</span> <span class="hljs-attr">value</span>=<span class="code-string">"Submit"</span> /&gt;</span>\n    <span class="code-tag">&lt;/<span class="code-name">form</span>&gt;</span>\n  );\n}</code></pre>\n<p>例如中“defaultValue = "Bob"”就是指定了一个默认值。同样地，&nbsp;<code>&lt;input type="checkbox"&gt;</code>&nbsp;和&nbsp;<code>&lt;input type="radio"&gt;</code>&nbsp;支持&nbsp;<code>defaultChecked</code>属性，&nbsp;<code>&lt;select&gt;</code>&nbsp;标签支持&nbsp;<code>defaultValue</code>属性。</p>'},397:function(s,n,a){"use strict";Object.defineProperty(n,"__esModule",{value:!0});n.content='<p>本文是为既没有机器学习基础也没了解过TensorFlow的码农、序媛们准备的。如果已经了解什么是MNIST和softmax回归本文也可以再次帮助你提升理解。在阅读之前，请先确保在合适的环境中安装了TensorFlow（<a\n        title="windows安装tensorflow" href="https://www.chkui.com/article/tensorflow/tensorflow_windows_install">windows安装请点这里</a>，其他版本请官网找），适当编写文章中提到的例子能提升理解。\n</p>\n<p>首先我们需要了解什么是“<strong>MNIST</strong>”?</p>\n<p>每当我们学习一门新的语言时，所有的入门教程官方都会提供一个典型的例子——“Hello World”。而在机器学习中，入门的例子称之为MNIST。</p>\n<p>MNIST是一个简单的视觉计算数据集，它是像下面这样手写的数字图片：</p>\n<p><img align="left" alt="TensorFlow MNIST机器学习入门" height="100"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_1.png" width="400"></p>\n<p></p>\n<p></p>\n<p></p>\n<p>每张图片还额外有一个标签记录了图片上数字是几，例如上面几张图的标签就是：5、0、4、1。</p>\n<p>本文将会展现如何训练一个模型来识别这些图片，最终实现模型对图片上的数字进行预测。</p>\n<p>首先要明确，我们的目标并不是要训练一个能在实际应用中使用的模型，而是通过这个过程了解如何使用TensorFlow完成整个机器学习的过程。我们会从一个非常简单的模型开始——Softmax回归。</p>\n<p>然后要明白，例子对应的源代码非常简单，所有值得关注的信息仅仅在三行代码中。然而，这对于理解TensorFlow如何工作以及机器学习的核心概念非常重要，因此本文除了说明原理还会详细介绍每一行代码的功用。</p>\n\n<h2 id="h2-1">阅读前需要了解的</h2>\n<p>阅读之前请先获取&nbsp;<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py"\n                    rel="nofollow">mnist_softmax.py</a>&nbsp;的代码，文中会一步一步的介绍每一行代码的内容。建议读者分为2种方式去学习后面的内容：</p>\n<ol>\n    <li>跟随本文的介绍一行接着一行的将代码片段拷贝到python开发环境中，边阅读边理解代码的含义。</li>\n    <li>稍微有点基础的，可以在继续向下阅读之前先理解和运行下&nbsp;<code>mnist_softmax.py</code>&nbsp;的代码，之后回来继续阅读以解惑不明部分。</li>\n</ol>\n<p>本文包含以下内容：</p>\n<ol>\n    <li>学习MNIST的数据解析以及softmax回归算法。</li>\n    <li>创建一个基于图片像素识别图片数字的模型。</li>\n    <li>使用TensorFlow来训练模型识别数字，这个学习的过程是让它去看成千上万的图片。</li>\n    <li>使用我们的测试数据来验证模型的准确性。</li>\n</ol>\n\n<h2 id="h2-2">模型创建</h2>\n\n<h3 id="h3-1">MINIST数据</h3>\n<p>MINIST数据集的官网是&nbsp;<a href="http://yann.lecun.com/exdb/mnist/" rel="nofollow">Yann LeCun\'s website</a>。下面这2行代码的作用是从MINIST官网自动下载并读取数据：\n</p>\n<pre class="python"><code class="language-python"><span class="code-keyword"><span\n        class="code-keyword">from</span></span> tensorflow.examples.tutorials.mnist <span class="code-keyword"><span\n        class="code-keyword">import</span></span> input_data\nmnist = input_data.read_data_sets(<span class="code-string"><span class="code-string">"MNIST_data/"</span></span>, one_hot=<span\n            class="code-keyword"><span class="code-keyword">True</span></span>)</code></pre>\n<p>\n    MINIST的数据分为2个部分：55000份训练数据（mnist.train）和10000份测试数据（mnist.test）。这个划分有重要的象征意义，他展示了在机器学习中如何使用数据。在训练的过程中，我们必须单独保留一份没有用于机器训练的数据作为验证的数据，这才能确保训练的结果是可以在所有范围内推广的（可泛化）。</p>\n<p>\n    前面已经提到，每一份MINIST数据都由图片以及标签组成。我们将图片命名为“x”，将标记数字的标签命名为“y”。训练数据集和测试数据集都是同样的结构，例如：训练的图片名为&nbsp;<code>mnist.train.images</code>&nbsp;而训练的标签名为&nbsp;<code>mnist.train.labels</code>。\n</p>\n<p>每一个图片均为28×28像素，我们可以将其理解为一个二维数组的结构：</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="158"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_2.png" width="400"></p>\n<p>\n    然后将这个数组扁平化成由784（28×28）个数字组成的一维数组。怎么扁平化数组并不重要，关键在于我们要保证所有的图片都使用一致的扁平化方法。因此，在数学意义上可以把MINIST图片看成是在784维度向量空间中的点，关于这个数据结构的复杂性可以参看这篇论文加以了解——<a\n        href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" rel="nofollow">Visualizing MNIST: An Exploration\n    of Dimensionality Reduction</a>。</p>\n<p>扁平化会丢失图片的二维结构信息，优秀的图形结构算法都会利用二维结构信息，但是为了简化过程便于理解，这里先使用这种一维结构来进行softmax回归。</p>\n<p><code>mnist.train.images</code>&nbsp;是一个形态为 &nbsp;<code>[55000, 784]</code>&nbsp;的张量（tensor）。第一个维度表示图片个数的索引，第二个维度表示图片中每一个像素的索引。每一个像素的取值为0或1，表示该像素的亮度。&nbsp;<code>mnist.train.images</code>&nbsp;可以理解为下图这样的空间结构：\n</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="181"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_3.png" width="400"></p>\n<p>MNIST的每一张图片都有一个数值0~9的标记。教程中，我们将标签数据设置为“one-hot vectors”。“one-hot\n    vectors”是指一个向量只有一个维度的数据是1，其他维度的数据都是0。在本文例子中，标记数据的维度将设置为1，而其他维度设置为0。例如3的向量结构是[0,0,0,1,0,0,0,0,0]。所以&nbsp;<code>mnist.train.labels</code>&nbsp;是一个结构为&nbsp;<code>[55000,\n        10]</code>&nbsp;的张量。</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="138"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_4.png" width="400"></p>\n<p>然后开始实际创建前面描述的模型。</p>\n\n<h3 id="h3-2">Softmax回归</h3>\n<p>\n    MNIST中每一张图片表示一个手写体0到9的数字，所以每一张图片所要表达的内容只有10种可能性。我们希望得到图片代表某个数字的概率。举个例子，一个模型当图片上的手写体数字是9时有80%的可能性识别的结果是9，还有5%的可能性识别出的结果是8。因这2者并没有覆盖100%的可能性，所有还有其他数字可能会出现。这是一个典型的softmax回归案例。softmax回归的作用是可以将概率分配给几个不同的对象，softmax提供了一个值处于0到1之间的列表，而列表中的值加起来为1。</p>\n<p>一个softmax回归包含2步：首先根据输入的数据提取该输入属于各个分类的“证据”（evidence），然后将这个证据转换成一个概率值。</p>\n<p>为了收集证据（evidence）以便将给定的图片划定到某个固定分类，我们对图片的像素值进行加权求和。如果有很明显的证据表明某个像素点会导致图片不属于该类，则加权值为负值反之为正。</p>\n<p>下面的图片展示了一个模型经过学习后，图片上的每个像素点对于特定数字的权值。红色表示负数权值、蓝色表示正数权值：</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="251"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_5.png" width="500"></p>\n<p>在训练的过程中，我们需要设定额外的偏置量（bias）以排除在输入中引入的干扰数据。下面图表示证据的提取计算公示，对于分类i给定一个x的输入得到：</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="78"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_6.png" width="278"></p>\n<p>\n    这里i表示分类（i=[0...9]），j表示图片x的像素索引（j=[0...784]）、Wij表示分类i在像素点j的加权值、xj表示图片x在j像素点的值（xj=[0,1]），bi表示分类i的偏移量。然后用softmax函数将这些证据转换成一个概率值：</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="53"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_7.png" width="276"></p>\n<p>\n    这里的softmax可以看成是一个转换函数，把线性函数的输出转换成需要的格式。在本文的例子中输出的就是图片在0~9这10个数字上的概率分布。因此，整个过程就是给定一个图片x，softmax最终会转化输出成一个对应每个分类概率值。它的定义是：</p>\n<p><img height="61" src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_8.png" width="350"\n        alt="TensorFlow MNIST机器学习入门"></p>\n<p>展开等式的子等式：</p>\n<p><img height="83" src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_9.png" width="337"\n        alt="TensorFlow MNIST机器学习入门"></p>\n<p>这个公式可以理解为：图片x在i分类中的加权值在所有加权值中的占比，exp()是e为底的指数计算公式。</p>\n<p><strong>上面的2个公示展示了softmax函数的计算过程</strong>：</p>\n<ol>\n    <li>\n        将参数作为幂运算的指数输入到公式中。使用幂指数的价值在于能够进一步放大（正值）或缩小（负值）权重值，对于设定的权重非常敏感。因为softmax使用幂指运算，所以再小的负值只会导致计算结果趋近于0，所以实际上运算结果不会出现负数或0。\n    </li>\n    <li>获取值之后，softmax对这些值进行归一化处理，使得在每个分类上形成有效的概率分布（保证每个分类的值在0到1之间，确保所有分类的和值为1）。</li>\n</ol>\n<p>如果想了解更多关于softmax回归的细节，请阅读<a href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax" rel="nofollow">Michael\n    Nieslen书中关于softmax的说明</a>。</p>\n<p>下图形象的展示了softmax回归的过程。Xj表示一个像素点（下图中j=[1,2,3]）。然后通过像素点与权重Wij的乘积求和（下图中i=[1,2,3]）再加上偏移量bi得到模型值，最后将模型进行softmax运算。</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="256"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_10.png" width="640"></p>\n<p>如果将其转换成线性方程式，得到：</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="146"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_11.png" width="640"></p>\n<p>还可以用向量矩阵来表述这个过程。下图的结果将上面的方程式转换成向量矩阵，这样有助于提升计算效率。</p>\n<p><img alt="TensorFlow MNIST机器学习入门" height="156"\n        src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_12.png" width="640"></p>\n<p>最后可以使用下面的等式形象的表示这个过程：</p>\n<p style="text-align:center">y=softmax(Wx+b)</p>\n<p style="text-align:left">接下来，我们将把这个过程使用TensorFlow实现。</p>\n\n<h3 id="h3-3">实现回归过程</h3>\n<p>为了在python中实现高效的数字运算建议用 <a href="http://www.numpy.org/" rel="nofollow">NumPy </a>这一类第三方库进行复杂的数学运算（比如下文会用到的矩阵乘法运算），因为它们使用了其他更高效的语言（C++）提升运算速度。使用外部库还是会存在一些效率问题：计算操作在python和其他语言之间来回切换会导致许多额外的性能开销，在使用GPU运算时这个问题尤为明显，在集群环境下同样存在这个问题。\n</p>\n<p>TensorFlow同样使用C++等语言实现高效的运算，但是为了避免前面说到的问题，TensorFlow进行了一些完善。在 <a\n        title="TensorFlow使用入门教程"\n        href="https://www.chkui.com/article/tensorflow/tensorflow_get_started">TensorFlow入门部分</a>\n    已经说明，它的执行过程是先构建模型然后再执行模型，所以TensorFlow会在模型执行期间一次性使用外部语言进行所有的计算然后再切换回python返回结果（绝大部分机器学习库都以这种方式实现）。</p>\n<p>首先，导入TensorFlow：</p>\n<pre class="haskell"><code class="language-python"><span class="code-keyword"><span\n        class="code-keyword">import</span></span> tensorflow <span class="code-keyword"><span\n        class="code-keyword">as</span></span> tf</code></pre>\n<p>声明变量x：</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">x</span> = tf.placeholder(tf.float32, [<span\n        class="code-keyword">None</span>, <span class="hljs-number"><span class="hljs-number">784</span></span>])</code></pre>\n<p>\n    x表示所有的手写体图片。它并不是一个固定值而是一个占位符，只有在TensorFlow运行时才会被设定真实值。根据前文描述的模型，输入是任意数量的MNIST图片，每张图片的像素值被扁平化到一个784维度的向量中（[784]），我们用一个二维浮点数张量来记录所有图片的输入，这个张量的形状就是&nbsp;<code>[None,\n    784]</code>&nbsp;。这里的&nbsp;<code>None</code>&nbsp;表示任意维度的向量。</p>\n<p>我们的模型还有&nbsp;<strong>权重&nbsp;</strong>和&nbsp;<strong>偏移量</strong>。由于是可训练数据，我们将这些值指定为一个附加输入，在 <a\n        title="TensorFlow使用入门教程"\n        href="https://www.chkui.com/article/tensorflow/tensorflow_get_started">TensorFlow入门部分</a>\n    我们称之为&nbsp;<strong>变量</strong>。变量就是可修改的张量，他在图中是一个可操作的节点。在计算的过程中，变量是样本训练的基础，通过不断调整变量来实现一个收敛过程找到变量的最佳值。\n</p>\n<pre class="ini"><code class="language-python"><span class="code-comment"><span class="code-comment"># tf.zeros表示所有的维度都为0 </span></span>\n<span class="hljs-attr">W</span> = tf.Variable(tf.zeros([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span\n            class="hljs-number"><span class="hljs-number">10</span></span>]))\n<span class="hljs-attr">b</span> = tf.Variable(tf.zeros([<span class="hljs-number"><span\n            class="hljs-number">10</span></span>]))</code></pre>\n<p>使用&nbsp;<code>tf.Variable</code>&nbsp;创建一个变量，然后使用&nbsp;<code>tf.zeros</code>&nbsp;将变量&nbsp;<code>W</code>&nbsp;和&nbsp;<code>b</code>&nbsp;设为值全为0的张量（就是将张量中的向量维度值设定为0）。由于我们在后续的过程中要使用大量的数据来训练&nbsp;<code>W</code>&nbsp;和&nbsp;<code>b</code>&nbsp;的值，因此他们的初始值是什么并不重要。\n</p>\n<p><strong>下面解释下W和b的意义</strong>：</p>\n<ul>\n    <li>\n        w的形状是一个[784,10]的张量，第一个向量列表表示每个图片都有784个像素点，第二个向量列表表示从“0”到“9”一共有10类图片。所以w用一个784维度的向量表示像素值，用10维度的向量表示分类，而2个向量进行乘法运算（或者说2个向量的笛卡尔集）就表示“某个像素在某个分类下的证据”。\n    </li>\n    <li>b的形状是[10]，他仅表示10个分类的偏移值。</li>\n</ul>\n<p>在TensorFlow中仅仅一行代码就可以实现整个运算过程：</p>\n<pre class="ini"><code class="ini"><span class="hljs-attr"><span class="hljs-attr">y</span></span> = tf.nn.softmax(tf.matmul(x, W) + b)</code></pre>\n<p><strong>解释一下这行代码</strong>：</p>\n<ol>\n    <li>&nbsp;<code>tf.matmul(x, W)</code>&nbsp;表达式表示W和x的乘积运算，对应之前等式（y=softmax(Wx+b)）的Wx，这个计算结果会得到一个y1=[None,\n        10]的张量，表示每个图片在10个分类下权重计算结果。\n    </li>\n    <li>&nbsp;<code>tf.matmul(x, W) + b</code>&nbsp;表示执行y2=y1+b的运算，它计算每个分类的偏移量。y2还是一个[None,10]的张量。</li>\n    <li>最后使用&nbsp;<code>tf.nn.softmax</code>&nbsp;进行归一计算，得到每张图片在每个分类下概率。</li>\n</ol>\n<p>\n    到止为止，使用TensorFlow完成了计算模型的创建。回忆下做了什么事：先用几行代码创建了数据（占位和变量），然后用一行代码创建了运算模型，代码非常的简短。简短并不是因为TensorFlow特意为softmax回归计算做了什么特别的设计，而是因为无论是机器学习建模还是物理仿真运算，使用TensorFlow描述数值计算都非常灵活。模型一旦定义好就可以在不同的设备上运行，例如电脑的CPU、GPU甚至手机。</p>\n\n<h2 id="h2-3">模型训练</h2>\n<p>\n    为了训练模型，需要针对模型定义一个指标来衡量模型“有多好”。不过事实上在机器学习中典型的方法是定义某个指标来衡量模型“有多差”。这个指标称为“成本”（cost）或“损益”（loss），它表示某个模型与期望的结果有多远。训练的目的就是不断的减少损益值，损益值越小证明我们的模型越好。</p>\n<p>通常情况下，“交叉熵”（cross-entropy）是非常适用于评估模型的损益值大小。交叉熵的概念来自于信息论的中关于信息压缩与编码的讨论，但是在博弈论、机器学习等其他许多领域也是重要的思想。他的数学定义是：</p>\n<p style="text-align:center"><img height="76"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_13.png"\n                                  width="240" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">q表示预测的概率分布，p表示真实分布（图片标签的分布）。简单的说，交叉熵就是描述当前模型距离真实的数据值还有多少差距。</p>\n\n<h3 id="h3-4">*信息论中的熵与交叉熵</h3>\n<blockquote>\n    <p>TensorFlow官网在对应的教程中并没有解释什么是交叉熵，这里根据我对信息论相关的数学知识理解说明什么是交叉熵。如果对数学符号所代表的含义没有兴趣，可以跳过本小节</p>\n</blockquote>\n\n<h4 id="h4-1">熵</h4>\n<p>\n    信息论中的熵是指信息不确定性的度量。熵越高代表为了呈现某个事物所需要的信息量就越多。熵的值都是基于概率的，其概率解释是：当某些事物在样本空间中的占比率越高，则需要将他描述出来的信息量越少。若占比率越少，则描述他的信息就越多。根据这个理论，熵值实质上衡量是信息量大小的数值，加上单位后的数量就是实际中常用于标记数据量大小的值，通常情况下用1位信息表示2个信号来表述，这个单位我们称之为bit。</p>\n<p>数学上对于均匀分布的信息，其熵可以用：</p>\n<p style="text-align:center"><img height="58"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_14.png"\n                                  width="117" alt="TensorFlow MNIST机器学习入门">&nbsp;（1）</p>\n<p>来表示。假设26个英文字符是均匀分布的，那么这里的p=1/26。若使用bit为运算单位，那么低x取值为2，所以计算得出26个英文字母需要4.7004bit信息来表述，取上界就是5bit的信息量。</p>\n<p>但是在现实世界中，绝大部分事物都不是均匀分布的。比如英文单词，通常字母e出现的频率比z就高很多，所以这种情况需要分别对不同事物的“统计特征”逐个计算，在这种情况下，熵的计算公式为：</p>\n<p style="text-align:center"><img height="66"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_16.png"\n                                  width="168" alt="TensorFlow MNIST机器学习入门">&nbsp;（2）</p>\n<p style="text-align:left">数学含义是：对于i个事物X，每个事物X的概率分布为Pi，其熵值为每个独立事物熵值占比的合计。</p>\n<p style="text-align:left">公式（1）是公式（2）在均匀分布下的推导：若P1=P2=P3=....=Pn且均分同一概率空间，那么可以推导出公式（1）。</p>\n<p style="text-align:left">举个例子。架设有4个英文字母集合v=[A,B,C,D]，其对应的概率分布p=[1/2,1/2,0,0]，那么：</p>\n<p style="text-align:center"><img height="55"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_17.png"\n                                  width="348" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">将公式展开：</p>\n<p style="text-align:center"><img height="45"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_18.png"\n                                  width="270" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">\n    若取bit作为度量单位，那么x=2，则得到的结果是H=1。所以如果按照这样的分布，只需要1bit的信息就可以表述所有的信息（因为C和D根本就不会出现，而A或B只需要一位[0,1]来表述）。</p>\n\n<h4 id="h4-2">交叉熵</h4>\n<p style="text-align:left">在公式（2）中p表示所有事物的真实分布，但是在实际情况中并不一定准确的清晰所有样本的真实分布，信息论中用交叉熵来表示这种情况，其表达式就是前面出现的公式：</p>\n<p style="text-align:center"><img height="76"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_19.png"\n                                  width="240" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">q是预测分布，而p是真实分布。</p>\n<p style="text-align:left">还是使用上面的例子：集合v=[A,B,C,D,E]、p=[1/2,1/2,0,0]，预测分布我们使用均匀分布p=[1/4,1/4,1/4,1/4]，所以带入公式：</p>\n<p style="text-align:center"><img height="45"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_20.png"\n                                  width="292" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">若取bit作为度量单位，那么x=2，则交叉熵的值H(p,q)=2。</p>\n\n<h4 id="h4-3">交叉熵作为损益计算的意义</h4>\n<p>H(p,q)&gt;=H(p)恒成立，当q等于真实分布q时取等号。因此在机器学习中，若p表示真实标记的分布，q为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。</p>\n<p>为了在编码中实现交叉熵，首先需要增加一个占位符来输入真实分布值：</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">y_</span> = tf.placeholder(tf.float32, [<span\n        class="code-keyword">None</span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code></pre>\n<p>然后我们实现交叉熵功能：</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">cross_entropy</span> = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[<span\n        class="hljs-number"><span class="hljs-number">1</span></span>]))</code></pre>\n<p>这一行代码的含义：</p>\n<ol>\n    <li>使用&nbsp;<code>tf.log</code>&nbsp;对y进行对数计算。</li>\n    <li>然后用y_与&nbsp;<code>tf.log</code>&nbsp;相乘。</li>\n    <li>用&nbsp;<code>tf.reduce_sum</code>&nbsp;根据&nbsp;<code>reduction_indices=[1]</code>&nbsp;指定的参数，计算y中第二个维度所有元素的总和。\n    </li>\n    <li>最后,&nbsp;<code>tf.reduce_mean</code>&nbsp;用于计算该批次的一个平均值。</li>\n</ol>\n<p>\n    需要注意的是，这里的cross_entropy仅仅用于示例以便于理解。在源代码中，不推荐使用这个公式，因为在数值上非常不稳定。推荐使用<code>tf.nn.softmax_cross_entropy_with_logits</code>&nbsp;&nbsp;方法直接训练&nbsp;&nbsp;<code>tf.matmul(x,\n    W) + b)</code>&nbsp;的结果。因为他在内部计算softmax，数值更加稳定。</p>\n<p>现在，我们已经定义好模型和训练方式，TensorFlow接下来会使用“<a href="http://colah.github.io/posts/2015-08-Backprop/" rel="nofollow">反转传播算法</a>”逐步修改变量以找到能使损益值最小化的变量值。现在定义所需的优化器：\n</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">train_step</span> = tf.train.GradientDescentOptimizer(<span\n        class="hljs-number"><span class="hljs-number">0.5</span></span>).minimize(cross_entropy)</code></pre>\n<p>\n    上面的代码设定梯度递减算法为训练的优化器，每次变量的调整范围为0.01。梯度递减是非常快捷高效的算法，TensorFlow需要做的是将每个变量一点点的向损益值变小的方向逐渐移动。TensorFlow提供了非常多的优化算法，只需要修改这行代码即可将优化器修改为其他算法。</p>\n<p>整个过程TensorFlow都做了什么呢？在另外一篇 <a title="TensorFlow使用入门教程"\n                                 href="https://www.chkui.com/article/tensorflow/tensorflow_get_started">TensorFlow入门</a>\n    的博文中介绍了TensorFlow的计算过程就是一个图。在后台运算时，TensorFlow会向这个图增加额外的操作以实现“反转传播算法”和“梯度递减算法”。最后TensorFlow返回一个独立的操作入口（就是上面代码中的train_step），这个入口会用梯度下降算法训练你的模型、微调你的变量、不断减少损益值。\n</p>\n<p>现在可以在&nbsp;<code>InteractiveSession</code>&nbsp;中启用该模型：</p>\n<pre class="ini"><code class="language-python"><span\n        class="hljs-attr">sess</span> = tf.InteractiveSession()</code></pre>\n<p>在运行之前需要初始化所有的变量：</p>\n<pre class="css"><code class="language-python"><span class="code-selector-tag">tf</span><span\n        class="code-selector-class">.global_variables_initializer</span>()<span class="code-selector-class">.run</span>()</code></pre>\n<p>然后就可以开始训练了，我们允许一次执行1000次训练。</p>\n<pre class="groovy"><code class="language-python"><span class="code-keyword"><span\n        class="code-keyword">for</span></span> _ <span class="code-keyword"><span class="code-keyword">in</span></span> range(<span\n        class="hljs-number"><span class="hljs-number">1000</span></span>):\n&nbsp; batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number"><span class="hljs-number">100</span></span>)\n&nbsp; sess.run(train_step, feed_dict={<span class="code-string">x:</span> batch_xs, <span\n            class="code-string">y_:</span> batch_ys})</code></pre>\n<p>\n    循环中的每一步，我们都会从训练图片分类的集合中（回想一下，前面提到每一个图片包含图片自身的像素值以及一个标记数值的标签）中随机取出100条随机数据，然后执行train_step将占位数据替换成从测试图片库mnist.train中获取的参数。</p>\n<p>使用一个小批量的随机数称为随机训练（stochastic\n    training），在这个例子中可以叫随机梯度递减训练。理想状态下，当然期望所有的数据都用于每一步训练，这样可以覆盖到所有样本，但是所带来的负面影响是计算成本太高。所以，每次训练都随机使用不同的数据子集，这样既降低计算成本又有效最大化利用数据。</p>\n\n<h2 id="h2-4">模型评估</h2>\n<p>现在我们来到了所有工作的最后一步，验证所设计的模型是否足够好。</p>\n<p>\n    首先找出那些被模型预测正确的图标。&nbsp;<code>tf.argmax</code>&nbsp;是一个非常有用的方法，它能够找到张量中某个列表最高数值的条目索引。例如&nbsp;<code>tf.argmax(y,1)</code>&nbsp;是找到张量y第二个向量的最大值（图标标签是0~9,softmax计算完成后会得到一个分布概率，argmax方法就是找到每一个图片对应的最高概率），而&nbsp;<code>tf.argmax(y_,1)</code>&nbsp;设定的参数就是真实标签。然后使用&nbsp;<code>tf.equal</code>&nbsp;方法检查预测是否和真实情况一样。\n</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">correct_prediction</span> = tf.equal(tf.argmax(y,<span\n        class="hljs-number"><span class="hljs-number">1</span></span>), tf.argmax(y_,<span class="hljs-number"><span\n        class="hljs-number">1</span></span>))</code></pre>\n<p>\n    比较的结果会返回一个boolean列表，为了确定正确预测的占比，我们用float类型来表示boolean类型。例如对于boolean类型的列表[true,false,true,true]可以转换成[1,0,1,1]，然后正确占比是75%。下面的代码完成了这一步工作。</p>\n<pre class="ini"><code class="language-python"><span class="hljs-attr">accuracy</span> = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></pre>\n<p>最后，运行它测试模型预测图片数据的准确率。</p>\n<pre class="less"><code class="language-python"><span class="code-comment"># 这里使用的是整个<span class="code-selector-tag">mnist</span><span\n        class="code-selector-class">.test</span>的数据</span>\n<span class="code-selector-tag">print</span>(sess.run(accuracy, feed_dict={<span class="code-attribute">x</span>: mnist.test.images, <span\n            class="code-attribute">y_</span>: mnist.test.labels}))</code></pre>\n<p>输出的结果应该是92%左右。</p>\n<p>\n    最后我们说说这个结果。从严格意义上来说，92%的正确率是非常差的。因为我们所使用的模型不仅非常简单而且数据也非常规范。不过不必灰心，只要经过小小的跳转，可以让这个正确率立刻达到97%，而现在最好的模型达到了99.7%。关于模型准确性的讨论，可以看\n    <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"\n       rel="nofollow">这里</a>。</p>\n<p>本文的价值并不是正确率，而是了解如何进行机器学习建模。当然，对于机器学习这仅仅是一个开始，后续会在这篇文章的基础上继续介绍如何使用TensorFlow搭建更复杂更有价值的模型。</p>\n\n<h2 id="h2-5">附记</h2>\n<p>在写本文时，正好在微信朋友圈和OC都看到传得正火爆的<a href="https://www.oschina.net/news/84215/talk-about-ai" rel="nofollow">《自动编程是不可能的\n    我为什么不在乎人工智能》</a>一文。关于人工智能是否值得在乎每个人都有自己的看法谈下去没什么价值，不过自动编程这一点倒是在这一代AI技术上不太可能做得比普通工程师做得更好。原因很简单，本文提到的一个数学公式就说明了这个问题：\n</p>\n<p style="text-align:center"><img height="76"\n                                  src="https://file.mahoooo.com/res/file/tensorflow_get_started_of_mnist_21.png"\n                                  width="240" alt="TensorFlow MNIST机器学习入门"></p>\n<p style="text-align:left">这是信息交叉熵公式，也是机器学习常用的损益评估公式。当期望分布p=q时，获得最少信息量或最少损益值，收敛学习结果的过程，其实就是在找p=q或让q逐渐接近p的过程。</p>\n<p style="text-align:left">\n    从信息论的角度来看，所有用户通过自然语言描述的需求对于机器来说熵值是非常高的，因为大段的自然语言中有用的内容在整体信息中的分布非常低。而通过产品人和研发工程师的转换最终成为熵值非常低的源码。所以靠不懂编程开发的人去实现有价值的程序个人认为几乎不可能。</p>\n<p style="text-align:left">\n    不过AI对于编程肯定有增益效果。比如当年用C#写windows界面，妥妥拽拽代码就生成好了，码农下一步活就是在各种回调事件中写业务、写DAO、写调用。AI也可以通过大数据观察各种功能的实现方式，最终帮码农快速生成一个60~70%可用的代码，然后序员们在以后的基础上继续写逻辑。个人觉得这个思路可以推广到很多行业——减少重复劳动，增加有特定意义的劳动时间，最终实现提升生产率。</p>'}});